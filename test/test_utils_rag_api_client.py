#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºè¿œç¨‹ RAG API çš„çŸ¥è¯†åº“æ„å»ºæµ‹è¯•ï¼ˆé€æ–‡ä»¶è¿‡ç¨‹å±•ç¤ºï¼‰
- ä»“åº“ç¤ºä¾‹ï¼šdata/repos/CLIP/
- ä»…æ‰“å°ä¸­æ–‡è¿‡ç¨‹è¯´æ˜ï¼›ä¸ä¼šçœŸæ­£è¿è¡Œäºæ­¤ç¯å¢ƒä¸­
- è¿è¡Œæ–¹å¼ï¼ˆæœ¬åœ°ï¼‰ï¼špython -m test.test_utils_rag_api_client
"""

import os
import sys
import asyncio
import json

from pathlib import Path

# ç¡®ä¿å¯ä»¥ä» src å¯¼å…¥
PROJECT_ROOT = Path(__file__).resolve().parents[1]
SRC_DIR = PROJECT_ROOT / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

from utils.rag_api_client import RAGVectorStoreProvider  # type: ignore # noqa: E402


BASE_URL = os.getenv("RAG_BASE_URL", "http://nodeport.sensedeal.vip:32421")
REPO_DIR = PROJECT_ROOT / "data" / "repos" / "CLIP"
REPO_FULL_NAME = "openai/CLIP"  # ä»…ç”¨äºç´¢å¼•å‘½åä¸å…ƒä¿¡æ¯

# ä¸å®ç°ä¿æŒä¸€è‡´ï¼Œç”¨äºåŒºåˆ†â€œæ–‡æ¡£/ä»£ç â€ç±»åˆ«çš„ç®€å•è§„åˆ™
DOC_EXTS = {".md", ".mdx", ".rst", ".txt", ".adoc"}


async def run_test():
    print("ğŸš€ è¿œç¨‹ RAG API çŸ¥è¯†åº“æ„å»ºæµ‹è¯•ï¼ˆé€æ–‡ä»¶é¢„è§ˆï¼‰")
    print("ä»“åº“è·¯å¾„:", REPO_DIR)
    print("æœåŠ¡åœ°å€:", BASE_URL)

    if not REPO_DIR.exists():
        print("âš ï¸ æœ¬åœ°ä»“åº“ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆåœ¨ data/repos/ ä¸‹å‡†å¤‡ CLIP ä»“åº“å‰¯æœ¬ã€‚")
        return

    # å®ä¾‹åŒ– Providerï¼ˆè¿œç¨‹ RAG API æ–¹æ¡ˆï¼‰
    provider = RAGVectorStoreProvider(BASE_URL)

    # å¥åº·æ£€æŸ¥ï¼ˆä¸å¼ºåˆ¶é€€å‡ºï¼Œä»…æç¤ºï¼‰
    ok = provider.rag_client.check_health()
    print("ğŸ” å¥åº·æ£€æŸ¥:", "é€šè¿‡" if ok else "æœªé€šè¿‡ï¼ˆè¯·ç¡®è®¤æœåŠ¡å¯ç”¨ï¼‰")

    # é€æ–‡ä»¶é¢„æ‰«æä¸å…ƒç´ æå–ï¼ˆä»…ç”¨äºæ‰“å°è¿‡ç¨‹ï¼Œä¸ä¼šæå‰æäº¤ï¼‰
    print("\n=== é€æ–‡ä»¶é¢„è§ˆï¼ˆæå–å…ƒç´ ï¼Œä»…æ‰“å°ï¼‰ ===")
    total_docs = 0
    all_documents = []

    try:
        files = provider._get_code_files(REPO_DIR)  # ä½¿ç”¨ç›¸åŒçš„æ–‡ä»¶è¿‡æ»¤è§„åˆ™
        for idx, file_path in enumerate(sorted(files), 1):
            try:
                rel = str(file_path.relative_to(REPO_DIR))
            except Exception:
                rel = str(file_path)

            category = "æ–‡æ¡£" if file_path.suffix.lower() in DOC_EXTS else "ä»£ç "
            elements = provider.code_splitter.extract_code_elements(file_path)
            n = len(elements)
            total_docs += n

            # ç»„è£…å®Œæ•´æ–‡æ¡£ç»“æ„ï¼Œå‡†å¤‡ä¿å­˜ä¸º JSON
            for element in elements:
                if element.get("element_type") in ("function", "class"):
                    desired_title = element.get("element_name", element.get("title", ""))
                else:
                    desired_title = element.get("title") or element.get("element_name") or file_path.stem
                try:
                    file_rel_for_json = str(file_path.relative_to(REPO_DIR))
                except Exception:
                    file_rel_for_json = element.get("file_path", str(file_path))
                desired_category = "æ–‡æ¡£" if file_path.suffix.lower() in DOC_EXTS else "ä»£ç "
                doc = {
                    "title": desired_title,
                    "file": file_rel_for_json,
                    "content": element.get("content", ""),
                    "category": desired_category,
                    "language": element.get("language"),
                    "repo_name": REPO_FULL_NAME,
                    "start_line": element.get("start_line", 1),
                    "end_line": element.get("end_line", 1),
                }
                all_documents.append(doc)

            # å±•ç¤ºå‰ä¸¤ä¸ªæ ‡é¢˜ç¤ºä¾‹ï¼ˆå°½é‡å–å‡½æ•°/ç±»åï¼‰
            samples = []
            for e in elements[:2]:
                title = e.get("element_name") or e.get("title") or file_path.stem
                samples.append(title)

            sample_str = ", ".join(samples) if samples else "-"
            print(f"{idx:03d}. ğŸ“„ {rel} | ç±»åˆ«: {category} | å…ƒç´ : {n} | ç¤ºä¾‹: {sample_str}")
    except Exception as e:
        print("âŒ é¢„æ‰«æé˜¶æ®µå‡ºé”™:", str(e))
        return

    # ä¿å­˜æ‰€æœ‰ documents ä¸º JSON åˆ°å½“å‰æµ‹è¯•æ–‡ä»¶æ‰€åœ¨ç›®å½•
    try:
        out_path = Path(__file__).parent / f"documents_{REPO_DIR.name}.json"
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(all_documents, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å·²ä¿å­˜æ‰€æœ‰ documents åˆ°: {out_path} (å…± {len(all_documents)} æ¡)")
    except Exception as e:
        print("âš ï¸ ä¿å­˜ documents JSON å¤±è´¥:", str(e))

    # å®é™…æäº¤æ„å»ºï¼ˆè¿œç¨‹ RAG æœåŠ¡å†…éƒ¨ä¼šæŒ‰æ‰¹æ¬¡åˆ›å»º/è¿½åŠ ï¼‰
    print("\n=== å¼€å§‹åˆ›å»ºçŸ¥è¯†åº“ï¼ˆæŒ‰æ‰¹æ¬¡æäº¤ï¼‰ ===")
    print(f"é¢„è®¡æ€»æ–‡æ¡£æ•°: {total_docs}ï¼ˆå®é™…æäº¤æ—¶ä»¥æ‰¹æ¬¡è®¡æ•°ä¸ºå‡†ï¼‰")

    shared_repo_info = {"full_name": REPO_FULL_NAME}

    try:
        index_name = await provider.build_vectorstore(REPO_DIR, shared_repo_info)
        print("\nâœ… åˆ›å»ºå®Œæˆ")
        print("ç´¢å¼•åç§°:", index_name)
        print("ä»“åº“æ ‡è¯†:", REPO_FULL_NAME)
    except Exception as e:
        print("\nğŸ’¥ åˆ›å»ºå¤±è´¥:", str(e))


if __name__ == "__main__":
    asyncio.run(run_test())
