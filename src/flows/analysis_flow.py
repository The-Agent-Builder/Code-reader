"""
GitHub ä»“åº“åˆ†æä¸»æµç¨‹
æ•´åˆæ‰€æœ‰èŠ‚ç‚¹ï¼Œå®ç°å®Œæ•´çš„ä»£ç ä»“åº“è§£ææµç¨‹
"""

from typing import Dict, Any, List
from pocketflow import AsyncFlow

# ç›´æ¥å¯¼å…¥éœ€è¦çš„èŠ‚ç‚¹ï¼Œé¿å… __init__.py çš„ä¾èµ–é—®é¢˜
from ..nodes.github_info_fetch_node import GitHubInfoFetchNode
from ..nodes.git_clone_node import GitCloneNode
from ..nodes.vectorize_repo_node import VectorizeRepoNode
from ..nodes.code_parsing_batch_node import CodeParsingBatchNode
from ..nodes.readme_analysis_node import ReadmeAnalysisNode
from ..nodes.save_results_node import SaveResultsNode
from ..nodes.save_to_mysql_node import SaveToMySQLNode
from ..utils.logger import logger


class GitHubAnalysisFlow(AsyncFlow):
    """GitHub ä»“åº“åˆ†æä¸»æµç¨‹"""

    def __init__(self):
        super().__init__()

        # åˆ›å»ºèŠ‚ç‚¹å®ä¾‹
        self.github_info_node = GitHubInfoFetchNode()
        self.git_clone_node = GitCloneNode()
        self.vectorize_node = VectorizeRepoNode()
        self.code_parse_node = CodeParsingBatchNode()
        self.readme_analysis_node = ReadmeAnalysisNode()
        self.save_results_node = SaveResultsNode()
        self.save_mysql_node = SaveToMySQLNode()

        # æ„å»ºæµç¨‹é“¾
        self._build_flow()

    def _build_flow(self):
        """æ„å»ºåˆ†ææµç¨‹"""
        # è®¾ç½®èµ·å§‹èŠ‚ç‚¹
        self.start(self.github_info_node)

        # æ„å»ºèŠ‚ç‚¹é“¾ï¼šGitHubä¿¡æ¯ -> å…‹éš† -> å‘é‡åŒ– -> ä»£ç è§£æ -> READMEåˆ†æ -> ä¿å­˜ç»“æœ
        (
            self.github_info_node
            >> self.git_clone_node
            >> self.vectorize_node
            >> self.code_parse_node
            >> self.readme_analysis_node
            >> self.save_results_node
            >> self.save_mysql_node
        )

        logger.info("GitHub analysis flow constructed")

    async def prep_async(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """æµç¨‹é¢„å¤„ç†"""
        logger.info("ğŸš€ ========== å¼€å§‹ GitHub ä»“åº“åˆ†ææµç¨‹ ==========")
        logger.info("ğŸ“‹ é˜¶æ®µ: æµç¨‹åˆå§‹åŒ– (GitHubAnalysisFlow.prep_async)")

        # éªŒè¯è¾“å…¥
        if "repo_url" not in shared:
            logger.error("âŒ ç¼ºå°‘ä»“åº“ URL")
            raise ValueError("Repository URL is required")

        repo_url = shared.get("repo_url")
        if repo_url is None:
            logger.error("âŒ ä»“åº“ URL ä¸º None")
            raise ValueError("Repository URL cannot be None")

        if not isinstance(repo_url, str):
            logger.error(f"âŒ ä»“åº“ URL ç±»å‹é”™è¯¯: {type(repo_url)}")
            raise ValueError("Repository URL must be a string")

        if not repo_url.strip():
            logger.error("âŒ ä»“åº“ URL ä¸ºç©º")
            raise ValueError("Repository URL cannot be empty")

        logger.info(f"ğŸ¯ ç›®æ ‡ä»“åº“: {repo_url}")
        logger.info("ğŸ“Š åˆ†ææ¨¡å¼: å®Œæ•´åˆ†æ (åŒ…å«å‘é‡åŒ–)")

        # åˆå§‹åŒ–å…±äº«çŠ¶æ€
        shared.setdefault("status", "processing")
        shared["current_stage"] = "initialization"

        logger.info("âœ… æµç¨‹åˆå§‹åŒ–å®Œæˆ")
        return shared

    async def post_async(self, shared: Dict[str, Any], prep_res: Dict[str, Any], exec_res: str) -> Dict[str, Any]:
        """æµç¨‹åå¤„ç†"""
        # æ›´æ–°æœ€ç»ˆçŠ¶æ€
        if "result_filepath" in shared:
            shared["status"] = "completed"
            logger.info(f"Analysis completed successfully: {shared['result_filepath']}")
        else:
            shared["status"] = "failed"
            logger.error("Analysis failed: no result file generated")

        return shared


class QuickAnalysisFlow(AsyncFlow):
    """å¿«é€Ÿåˆ†ææµç¨‹ï¼ˆè·³è¿‡å‘é‡åŒ–ï¼‰"""

    def __init__(self, batch_size: int = 5):
        super().__init__()

        # åˆ›å»ºèŠ‚ç‚¹å®ä¾‹ï¼ˆè·³è¿‡å‘é‡åŒ–èŠ‚ç‚¹ï¼‰
        self.github_info_node = GitHubInfoFetchNode()
        self.git_clone_node = GitCloneNode()
        self.code_parse_node = CodeParsingBatchNode(batch_size=batch_size)
        self.readme_analysis_node = ReadmeAnalysisNode()
        self.save_results_node = SaveResultsNode()
        self.save_mysql_node = SaveToMySQLNode()

        # æ„å»ºæµç¨‹é“¾
        self._build_flow()

    def _build_flow(self):
        """æ„å»ºå¿«é€Ÿåˆ†ææµç¨‹"""
        # è®¾ç½®èµ·å§‹èŠ‚ç‚¹
        self.start(self.github_info_node)

        # æ„å»ºèŠ‚ç‚¹é“¾ï¼šGitHubä¿¡æ¯ -> å…‹éš† -> ä»£ç è§£æ -> READMEåˆ†æ -> ä¿å­˜ç»“æœ
        (
            self.github_info_node
            >> self.git_clone_node
            >> self.code_parse_node
            >> self.readme_analysis_node
            >> self.save_results_node
            >> self.save_mysql_node
        )

        logger.info("Quick analysis flow constructed")

    async def prep_async(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """æµç¨‹é¢„å¤„ç†"""
        logger.info("ğŸš€ ========== å¼€å§‹ GitHub ä»“åº“å¿«é€Ÿåˆ†ææµç¨‹ ==========")
        logger.info("ğŸ“‹ é˜¶æ®µ: æµç¨‹åˆå§‹åŒ– (QuickAnalysisFlow.prep_async)")

        # éªŒè¯è¾“å…¥
        if "repo_url" not in shared:
            logger.error("âŒ Repository URL is missing from shared data")
            raise ValueError("Repository URL is required")

        repo_url = shared.get("repo_url")
        if repo_url is None:
            logger.error("âŒ Repository URL is None")
            raise ValueError("Repository URL cannot be None")

        if not isinstance(repo_url, str):
            logger.error(f"âŒ Repository URL is not a string: {type(repo_url)}")
            raise ValueError("Repository URL must be a string")

        if not repo_url.strip():
            logger.error("âŒ Repository URL is empty")
            raise ValueError("Repository URL cannot be empty")

        logger.info(f"ğŸ¯ ç›®æ ‡ä»“åº“: {repo_url}")
        logger.info("âš¡ åˆ†ææ¨¡å¼: å¿«é€Ÿåˆ†æ (è·³è¿‡å‘é‡åŒ–)")

        shared.setdefault("status", "processing")
        shared["current_stage"] = "initialization"

        logger.info("âœ… æµç¨‹åˆå§‹åŒ–å®Œæˆ")
        return shared

    async def post_async(self, shared: Dict[str, Any], prep_res: Dict[str, Any], exec_res: str) -> Dict[str, Any]:
        """æµç¨‹åå¤„ç†"""
        if "result_filepath" in shared:
            shared["status"] = "completed"
            logger.info(f"Quick analysis completed: {shared['result_filepath']}")
        else:
            shared["status"] = "failed"
            logger.error("Quick analysis failed")

        return shared


async def analyze_repository(
    repo_url: str, use_vectorization: bool = True, batch_size: int = 10, progress_callback=None
) -> Dict[str, Any]:
    """
    åˆ†æGitHubä»“åº“çš„ä¾¿æ·å‡½æ•°

    Args:
        repo_url: GitHubä»“åº“URL
        use_vectorization: æ˜¯å¦ä½¿ç”¨å‘é‡åŒ–ï¼ˆRAGï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (completed, current_file) å‚æ•°

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    # å‡†å¤‡å…±äº«æ•°æ®
    shared = {"repo_url": repo_url, "progress_callback": progress_callback}

    # é€‰æ‹©æµç¨‹
    if use_vectorization:
        flow = GitHubAnalysisFlow()
        # è®¾ç½®æ‰¹æ¬¡å¤§å°
        if hasattr(flow.code_parse_node, "batch_size"):
            flow.code_parse_node.batch_size = batch_size
    else:
        flow = QuickAnalysisFlow(batch_size=batch_size)

    try:
        # æ‰§è¡Œåˆ†ææµç¨‹
        result = await flow.run_async(shared)

        # è¿”å›å®Œæ•´çš„å…±äº«æ•°æ®
        return shared

    except Exception as e:
        logger.error(f"Analysis failed for {repo_url}: {str(e)}")
        shared["status"] = "failed"
        shared["error"] = str(e)
        return shared


# æµç¨‹å·¥å‚å‡½æ•°
def create_analysis_flow(flow_type: str = "full", **kwargs) -> AsyncFlow:
    """
    åˆ›å»ºåˆ†ææµç¨‹çš„å·¥å‚å‡½æ•°

    Args:
        flow_type: æµç¨‹ç±»å‹ ("full", "quick")
        **kwargs: æµç¨‹å‚æ•°

    Returns:
        åˆ†ææµç¨‹å®ä¾‹
    """
    if flow_type == "full":
        return GitHubAnalysisFlow(**kwargs)
    elif flow_type == "quick":
        return QuickAnalysisFlow(**kwargs)
    else:
        raise ValueError(f"Unknown flow type: {flow_type}")


# æ‰¹é‡åˆ†æå‡½æ•°
async def analyze_repositories_batch(
    repo_urls: List[str], use_vectorization: bool = True, batch_size: int = 5
) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡åˆ†æå¤šä¸ªä»“åº“

    Args:
        repo_urls: ä»“åº“URLåˆ—è¡¨
        use_vectorization: æ˜¯å¦ä½¿ç”¨å‘é‡åŒ–
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        åˆ†æç»“æœåˆ—è¡¨
    """
    import asyncio

    results = []

    # åˆ†æ‰¹å¤„ç†ä»“åº“
    for i in range(0, len(repo_urls), batch_size):
        batch_urls = repo_urls[i : i + batch_size]

        # å¹¶è¡Œåˆ†æå½“å‰æ‰¹æ¬¡
        tasks = [analyze_repository(url, use_vectorization, batch_size) for url in batch_urls]

        batch_results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in batch_results:
            if isinstance(result, Exception):
                logger.error(f"Batch analysis error: {str(result)}")
                results.append({"status": "failed", "error": str(result)})
            else:
                results.append(result)

        # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        if i + batch_size < len(repo_urls):
            await asyncio.sleep(2)

    return results
