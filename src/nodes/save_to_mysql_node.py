"""
SaveToMySQLNode - å°†åˆ†æžç»“æžœä¿å­˜åˆ° MySQL (SQLAlchemy)
æ”¯æŒä»Žæœ¬åœ°æ–‡ä»¶å’Œæµç¨‹å…±äº«æ•°æ®ä¸¤ç§æ–¹å¼è¯»å–æ•°æ®
Design: Node
"""

from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from pocketflow import Node

from ..utils.logger import logger
from ..utils.db import get_session, init_db
from ..utils.config import get_config
from ..models.mysql_models import Repository, AnalysisTask, FileAnalysis, SearchTarget, AnalysisItem


class SaveToMySQLNode(Node):
    """å°†åˆ†æžç»“æžœä¿å­˜åˆ° MySQL çš„èŠ‚ç‚¹ã€‚

    æ”¯æŒä¸¤ç§æ•°æ®æºæ¨¡å¼ï¼š
    1. ä»Žæµç¨‹å…±äº«æ•°æ®è¯»å– (åŽŸæœ‰æ¨¡å¼)
       éœ€è¦ shared ä¸­åŒ…å«ï¼š
       - repo_url
       - repo_info
       - code_analysis (CodeParsingBatchNode çš„è¾“å‡º)

    2. ä»Žæœ¬åœ°æ–‡ä»¶è¯»å– (æ–°å¢žæ¨¡å¼)
       éœ€è¦ shared ä¸­åŒ…å«ï¼š
       - results_path: ç»“æžœæ–‡ä»¶è·¯å¾„ï¼Œå¦‚ "data/results/PocketFlow"
       æˆ–è€…
       - repo_name: ä»“åº“åç§°ï¼Œå°†è‡ªåŠ¨ä»Ž RESULTS_PATH çŽ¯å¢ƒå˜é‡æž„å»ºè·¯å¾„
    """

    def __init__(self, use_local_files: bool = False):
        super().__init__()
        self.use_local_files = use_local_files
        self.config = get_config()
        # ç¡®ä¿è¡¨å­˜åœ¨
        init_db()

    def prep(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        logger.info("ðŸ“‹ é˜¶æ®µ: å°†ç»“æžœä¿å­˜åˆ° MySQL (SaveToMySQLNode)")
        shared["current_stage"] = "saving_to_mysql"

        if self.use_local_files:
            # ä»Žæœ¬åœ°æ–‡ä»¶è¯»å–æ¨¡å¼
            logger.info("ðŸ—‚ï¸ ä½¿ç”¨æœ¬åœ°æ–‡ä»¶æ¨¡å¼è¯»å–åˆ†æžç»“æžœ")

            # ç¡®å®šç»“æžœæ–‡ä»¶è·¯å¾„
            if "results_path" in shared:
                results_path = Path(shared["results_path"])
            elif "repo_name" in shared:
                results_base_path = Path(self.config.results_path)
                results_path = results_base_path / shared["repo_name"]
            else:
                raise ValueError("æœ¬åœ°æ–‡ä»¶æ¨¡å¼éœ€è¦æä¾› 'results_path' æˆ– 'repo_name'")

            if not results_path.exists():
                raise ValueError(f"ç»“æžœæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {results_path}")

            shared["_results_path"] = results_path
            logger.info(f"ðŸ“ ç»“æžœæ–‡ä»¶è·¯å¾„: {results_path}")
        else:
            # ä»Žæµç¨‹å…±äº«æ•°æ®è¯»å–æ¨¡å¼ (åŽŸæœ‰é€»è¾‘)
            logger.info("ðŸ”„ ä½¿ç”¨æµç¨‹å…±äº«æ•°æ®æ¨¡å¼")
            required = ["repo_url", "repo_info", "code_analysis"]
            for k in required:
                if k not in shared:
                    raise ValueError(f"Missing required shared data: {k}")

        return shared

    def _load_local_files(self, results_path: Path) -> Dict[str, Any]:
        """ä»Žæœ¬åœ°æ–‡ä»¶åŠ è½½åˆ†æžç»“æžœæ•°æ®"""
        logger.info("ðŸ“– å¼€å§‹ä»Žæœ¬åœ°æ–‡ä»¶åŠ è½½åˆ†æžç»“æžœ")

        # å®šä¹‰éœ€è¦çš„æ–‡ä»¶
        files_to_load = {
            "repo_info": "repo_info.json",
            "metadata": "metadata.json",
            "analysis_report": "analysis_report.json",
        }

        loaded_data = {}

        # åŠ è½½å„ä¸ªæ–‡ä»¶
        for key, filename in files_to_load.items():
            file_path = results_path / filename
            if file_path.exists():
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        loaded_data[key] = json.load(f)
                    logger.info(f"âœ… æˆåŠŸåŠ è½½ {filename}")
                except Exception as e:
                    logger.warning(f"âš ï¸ åŠ è½½ {filename} å¤±è´¥: {e}")
                    loaded_data[key] = {}
            else:
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                loaded_data[key] = {}

        # æž„å»ºç»Ÿä¸€çš„æ•°æ®ç»“æž„
        repo_info = loaded_data["repo_info"]
        metadata = loaded_data["metadata"]
        analysis_report = loaded_data["analysis_report"]

        # æå–ä»“åº“ä¿¡æ¯
        repo_url = repo_info.get("url") or repo_info.get("clone_url", "")

        # æå–æ–‡ä»¶åˆ†æžæ•°æ®
        files_data = analysis_report.get("files", [])

        # è½¬æ¢ä¸ºä¸Žæµç¨‹æ•°æ®å…¼å®¹çš„æ ¼å¼
        code_analysis = []
        for file_data in files_data:
            file_analysis = {
                "file_path": file_data.get("file_path", ""),
                "language": file_data.get("language", "unknown"),
                "analysis_items": file_data.get("analysis_items", []),
                "error": None,  # æœ¬åœ°æ–‡ä»¶ä¸­é€šå¸¸ä¸åŒ…å«é”™è¯¯ä¿¡æ¯
            }
            code_analysis.append(file_analysis)

        result = {
            "repo_url": repo_url,
            "repo_info": repo_info,
            "code_analysis": code_analysis,
            "metadata": metadata,
            "analysis_report": analysis_report,
        }

        logger.info(f"ðŸ“Š åŠ è½½å®Œæˆ: {len(code_analysis)} ä¸ªæ–‡ä»¶çš„åˆ†æžç»“æžœ")
        return result

    def exec(self, shared: Dict[str, Any]) -> str:
        session_maker = get_session()
        session = session_maker()
        try:
            # æ ¹æ®æ¨¡å¼èŽ·å–æ•°æ®
            if self.use_local_files:
                # ä»Žæœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®
                results_path = shared["_results_path"]
                data = self._load_local_files(results_path)
                repo_info = data["repo_info"]
                code_analysis = data["code_analysis"]
                repo_url = data["repo_url"]
                metadata = data.get("metadata", {})
            else:
                # ä»Žæµç¨‹å…±äº«æ•°æ®èŽ·å– (åŽŸæœ‰é€»è¾‘)
                repo_info = shared.get("repo_info", {})
                code_analysis: List[Dict[str, Any]] = shared.get("code_analysis", [])
                repo_url = shared.get("repo_url", "")
                metadata = {}

            # 1) upsert Repository
            repo = Repository(
                user_id=1,  # é»˜è®¤ç”¨æˆ·IDï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
                name=repo_info.get("name") or repo_info.get("full_name", "unknown").split("/")[-1],
                full_name=repo_info.get("full_name"),
                local_path=shared.get("local_path", ""),  # ä»Žå…±äº«æ•°æ®èŽ·å–æœ¬åœ°è·¯å¾„
                status=1,  # é»˜è®¤çŠ¶æ€ä¸ºå­˜åœ¨
            )
            session.add(repo)
            session.flush()  # èŽ·å– repo.id

            # 2) create AnalysisTask
            # ä»Ž metadata æˆ– shared ä¸­èŽ·å–æ—¶é—´ä¿¡æ¯
            if self.use_local_files and metadata:
                analysis_time_str = metadata.get("analysis_time") or metadata.get("created_at")
                if analysis_time_str:
                    try:
                        # å°è¯•è§£æž ISO æ ¼å¼æ—¶é—´
                        analysis_time = datetime.fromisoformat(analysis_time_str.replace("Z", "+00:00"))
                    except:
                        analysis_time = datetime.now()
                else:
                    analysis_time = datetime.now()

                # ä»Ž metadata èŽ·å–ç»Ÿè®¡ä¿¡æ¯
                stats = metadata.get("statistics", {})
                total_files = stats.get("total_files", len(code_analysis))
                successful_files = len([r for r in code_analysis if not r.get("error")])
                failed_files = total_files - successful_files
            else:
                analysis_time = datetime.now()
                total_files = len(code_analysis)
                successful_files = sum(1 for r in code_analysis if not r.get("error"))
                failed_files = sum(1 for r in code_analysis if r.get("error"))

            task = AnalysisTask(
                repository_id=repo.id,
                total_files=total_files,
                successful_files=successful_files,
                failed_files=failed_files,
                code_lines=0,  # å¯ä»¥æ ¹æ®å®žé™…éœ€è¦è®¡ç®—
                module_count=0,  # å¯ä»¥æ ¹æ®å®žé™…éœ€è¦è®¡ç®—
                status="completed",
                start_time=analysis_time,
                end_time=analysis_time,
            )
            session.add(task)
            session.flush()

            # 3) create FileAnalysis + SearchTarget + AnalysisItem
            for file_res in code_analysis:
                file_path = file_res.get("file_path")
                language = file_res.get("language") or "unknown"
                err = file_res.get("error")

                file_row = FileAnalysis(
                    task_id=task.id,
                    file_path=file_path,
                    language=language,
                    status=("failed" if err else "success"),
                    error_message=(str(err) if err else None),
                )
                session.add(file_row)
                session.flush()

                items = file_res.get("analysis_items", [])
                # å°è¯•è¯»å–å¸¦åˆ†ç»„çš„å½¢å¼ï¼ˆå¦‚æžœ code_parsing èŠ‚ç‚¹å†™å…¥ shared æ—¶é™„å¸¦äº† search_targetï¼‰
                for item in items:
                    search_target_text = item.get("search_target") or item.get("file_path") or file_path
                    # è§£æž target ç±»åž‹
                    target_type = "file"
                    target_name: Optional[str] = None
                    if search_target_text.startswith("æ–‡ä»¶-ç±»("):
                        target_type = "class"
                        target_name = search_target_text[len("æ–‡ä»¶-ç±»(") : -1]
                    elif search_target_text.startswith("æ–‡ä»¶-å‡½æ•°("):
                        target_type = "function"
                        target_name = search_target_text[len("æ–‡ä»¶-å‡½æ•°(") : -1]
                    else:
                        target_type = "file"
                        target_name = file_path

                    target_identifier = search_target_text
                    target_row = SearchTarget(
                        file_analysis_id=file_row.id,
                        target_type=target_type,
                        target_name=target_name,
                        target_identifier=target_identifier,
                    )
                    session.add(target_row)
                    session.flush()

                    # AnalysisItem
                    ai = AnalysisItem(
                        file_analysis_id=file_row.id,
                        search_target_id=target_row.id,
                        title=item.get("title", "Unknown"),
                        description=item.get("description"),
                        source=item.get("source"),
                        language=item.get("language"),
                        code=item.get("code"),
                        start_line=_extract_start_line(item.get("source")),
                        end_line=_extract_end_line(item.get("source")),
                    )
                    session.add(ai)

            session.commit()
            logger.info(f"âœ… MySQL ä¿å­˜å®Œæˆ: repo_id={repo.id}, task_id={task.id}")
            return "default"
        except Exception as e:
            session.rollback()
            logger.error(f"âŒ ä¿å­˜åˆ° MySQL å¤±è´¥: {e}")
            raise
        finally:
            session.close()

    def post(self, shared: Dict[str, Any], _prep_res: Dict[str, Any], _exec_res: str) -> str:
        shared["mysql_saved"] = True
        data_source = "æœ¬åœ°æ–‡ä»¶" if self.use_local_files else "æµç¨‹æ•°æ®"
        logger.info(f"âœ… MySQL ä¿å­˜å®Œæˆ (æ•°æ®æº: {data_source})")
        return "default"


def _extract_start_line(source: Optional[str]) -> Optional[int]:
    if not source:
        return None
    try:
        if ":" in source:
            part = source.split(":", 1)[1]
            if "-" in part:
                return int(part.split("-", 1)[0])
            return int(part)
    except Exception:
        return None
    return None


def _extract_end_line(source: Optional[str]) -> Optional[int]:
    if not source:
        return None
    try:
        if ":" in source:
            part = source.split(":", 1)[1]
            if "-" in part:
                return int(part.split("-", 1)[1])
            return int(part)
    except Exception:
        return None
    return None
