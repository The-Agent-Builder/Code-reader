"""
WebCodeAnalysisNode - ä»æ•°æ®åº“è¯»å–æ–‡ä»¶å¹¶è¿›è¡ŒAIåˆ†æ
ä¸“é—¨ç”¨äºWebå‰ç«¯çš„"åˆ†ææ•°æ®æ¨¡å‹"æ¨¡å—
"""

import asyncio
import aiohttp
from typing import Dict, Any, List
from pathlib import Path
from pocketflow import AsyncParallelBatchNode

from ..utils.llm_parser import LLMParser
from ..utils.rag_api_client import RAGAPIClient
from ..utils.logger import logger
from ..utils.error_handler import LLMParsingError
from ..utils.config import get_config


class WebCodeAnalysisNode(AsyncParallelBatchNode):
    """Webä»£ç åˆ†æèŠ‚ç‚¹ - ä»æ•°æ®åº“è¯»å–æ–‡ä»¶å¹¶è¿›è¡ŒAIåˆ†æ"""

    def __init__(self, batch_size: int = None):
        super().__init__(max_retries=2, wait=20)
        self.llm_parser = LLMParser()  # LLMè§£æå™¨

        config = get_config()
        self.rag_client = RAGAPIClient(config.rag_base_url)  # RAG APIå®¢æˆ·ç«¯
        self.api_base_url = config.api_base_url  # åç«¯APIåœ°å€
        self.batch_size = batch_size if batch_size is not None else config.llm_batch_size
        self.max_concurrent = config.llm_max_concurrent

    async def prep_async(self, shared: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»æ•°æ®åº“è·å–æ–‡ä»¶åˆ—è¡¨å¹¶å‡†å¤‡åˆ†æå‚æ•°

        Data Access:
        - Read: shared.task_id, shared.vectorstore_index
        """
        logger.info("=" * 60)
        logger.info("ğŸ“‹ é˜¶æ®µ: Webä»£ç åˆ†æ (WebCodeAnalysisNode)")

        shared["current_stage"] = "web_code_analysis"

        task_id = shared.get("task_id")
        vectorstore_index = shared.get("vectorstore_index")

        if not task_id:
            logger.error("âŒ Webä»£ç åˆ†æéœ€è¦æä¾›ä»»åŠ¡ID")
            raise LLMParsingError("Task ID is required")

        if not vectorstore_index:
            logger.error("âŒ Webä»£ç åˆ†æéœ€è¦æä¾›å‘é‡ç´¢å¼•")
            raise LLMParsingError("Vectorstore index is required")

        logger.info(f"ğŸ” è·å–ä»»åŠ¡ {task_id} çš„æ–‡ä»¶åˆ—è¡¨")

        # 1. ä»APIè·å–æ–‡ä»¶åˆ—è¡¨ï¼ˆä¸åŒ…å«ä»£ç å†…å®¹ï¼‰
        file_list = await self._get_repository_files(task_id)
        if not file_list:
            logger.warning(f"ä»»åŠ¡ {task_id} æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
            return []

        logger.info(f"ğŸ“ æ‰¾åˆ° {len(file_list)} ä¸ªæ–‡ä»¶éœ€è¦åˆ†æ")

        # 2. ä¸ºæ¯ä¸ªæ–‡ä»¶å‡†å¤‡åˆ†æå‚æ•°
        file_items = []
        for file_info in file_list:
            file_analysis_id = file_info.get("id")
            file_path = file_info.get("file_path")
            language = file_info.get("language", "unknown")

            if not file_analysis_id or not file_path:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ–‡ä»¶ä¿¡æ¯: {file_info}")
                continue

            file_items.append(
                {
                    "file_analysis_id": file_analysis_id,
                    "file_path": file_path,
                    "language": language,
                    "task_id": task_id,
                    "vectorstore_index": vectorstore_index,
                    "shared": shared,  # ä¼ é€’å…±äº«æ•°æ®å¼•ç”¨
                }
            )

        logger.info(f"å‡†å¤‡åˆ†æ {len(file_items)} ä¸ªæ–‡ä»¶")

        # æ·»åŠ è¿›åº¦å›è°ƒä¿¡æ¯
        progress_callback = shared.get("progress_callback")
        for i, file_item in enumerate(file_items):
            file_item["progress_callback"] = progress_callback
            file_item["current_index"] = i

        return file_items

    async def exec_async(self, file_item: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªæ–‡ä»¶ï¼šè·å–ä»£ç å†…å®¹ -> RAGæ£€ç´¢ -> LLMåˆ†æ
        """
        try:
            file_analysis_id = file_item["file_analysis_id"]
            file_path = file_item["file_path"]
            task_id = file_item["task_id"]

            logger.info(f"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: {file_path} (ID: {file_analysis_id})")

            # 1. è·å–æ–‡ä»¶çš„å®Œæ•´ä»£ç å†…å®¹
            file_content = await self._get_file_content(file_analysis_id, task_id)
            if not file_content:
                logger.warning(f"æ— æ³•è·å–æ–‡ä»¶å†…å®¹: {file_path}")
                return {
                    "file_analysis_id": file_analysis_id,
                    "file_path": file_path,
                    "analysis_items": [],
                    "error": "æ— æ³•è·å–æ–‡ä»¶å†…å®¹",
                }

            # 2. ä½¿ç”¨RAGè·å–ç›¸å…³ä¸Šä¸‹æ–‡
            context = await self._get_rag_context(file_item, file_content)

            # 3. è°ƒç”¨LLMè¿›è¡Œè¯¦ç»†åˆ†æ
            result = await self.llm_parser.parse_code_file_detailed(
                file_path, file_content, file_item["language"], context
            )

            # 4. æ·»åŠ æ–‡ä»¶åˆ†æIDåˆ°ç»“æœä¸­
            if not result.get("error"):
                result["file_analysis_id"] = file_analysis_id
                # ä¸ºæ¯ä¸ªåˆ†æé¡¹æ·»åŠ file_analysis_id
                for item in result.get("analysis_items", []):
                    item["file_analysis_id"] = file_analysis_id

            # 5. è°ƒç”¨è¿›åº¦å›è°ƒ
            progress_callback = file_item.get("progress_callback")
            if progress_callback:
                try:
                    progress_callback(current_file=file_path)
                except Exception as e:
                    logger.warning(f"Progress callback failed: {str(e)}")

            logger.info(f"âœ… å®Œæˆæ–‡ä»¶åˆ†æ: {file_path}")
            return result

        except Exception as e:
            logger.error(f"Failed to analyze file {file_item['file_path']}: {str(e)}")
            return {
                "file_analysis_id": file_item["file_analysis_id"],
                "file_path": file_item["file_path"],
                "analysis_items": [],
                "error": str(e),
            }

    async def _get_repository_files(self, task_id: int) -> List[Dict[str, Any]]:
        """ä»APIè·å–ä»“åº“æ–‡ä»¶åˆ—è¡¨"""
        try:
            url = f"{self.api_base_url}/api/repository/files/{task_id}"
            params = {"include_code_content": False}

            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("files", [])
                    elif response.status == 404:
                        logger.warning(f"ä»»åŠ¡ {task_id} æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
                        return []
                    else:
                        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: HTTP {response.status}")
                        return []

        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []

    async def _get_file_content(self, file_analysis_id: int, task_id: int) -> str:
        """ä»APIè·å–å•ä¸ªæ–‡ä»¶çš„ä»£ç å†…å®¹"""
        try:
            url = f"{self.api_base_url}/api/repository/file-analysis/{file_analysis_id}"
            params = {"task_id": task_id}

            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        file_analysis = data.get("file_analysis", {})
                        return file_analysis.get("code_content", "")
                    else:
                        logger.error(f"è·å–æ–‡ä»¶å†…å®¹å¤±è´¥: HTTP {response.status}")
                        return ""

        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return ""

    async def _get_rag_context(self, file_item: Dict[str, Any], content: str) -> str:
        """
        ä½¿ç”¨RAG APIè·å–ä¸å½“å‰æ–‡ä»¶ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        å¤ç”¨CodeParsingBatchNodeçš„RAGé€»è¾‘
        """
        try:
            vectorstore_index = file_item.get("vectorstore_index")
            if not vectorstore_index:
                logger.warning("RAG API ç´¢å¼•ä¸å¯ç”¨")
                return ""

            file_path = file_item["file_path"]
            language = file_item["language"]

            # 1. æå–ç±»-æ–¹æ³•å…³è”å…³ç³»å’Œç‹¬ç«‹å‡½æ•°
            class_method_relationships = self._extract_class_method_relationships(content, language)
            independent_functions = self._extract_independent_functions(content, language, class_method_relationships)

            logger.info(f"ğŸ” ä» {file_path} æ–‡ä»¶ä¸­æå–åˆ°:")
            logger.info(f"   - ç±»: {list(class_method_relationships.keys())}")
            logger.info(f"   - ç‹¬ç«‹å‡½æ•°: {independent_functions}")

            # 2. æ„å»ºæ£€ç´¢ç­–ç•¥ï¼šæ–‡ä»¶ + æ¯ä¸ªç±» + æ¯ä¸ªç‹¬ç«‹å‡½æ•°
            search_queries = []
            search_targets = []

            # 2.1 å¯¹æ–‡ä»¶æœ¬èº«è¿›è¡Œæ£€ç´¢
            search_queries.append(f"{file_path} {language} æ–‡ä»¶")
            search_targets.append(f"æ–‡ä»¶-{file_path}")

            # 2.2 å¯¹æ¯ä¸ªç±»åˆ†åˆ«è¿›è¡Œæ£€ç´¢
            for class_name in class_method_relationships.keys():
                search_queries.append(f"{class_name} ç±» {language}")
                search_targets.append(f"æ–‡ä»¶-ç±»({class_name})")

            # 2.3 å¯¹æ¯ä¸ªç‹¬ç«‹å‡½æ•°åˆ†åˆ«è¿›è¡Œæ£€ç´¢
            for func_name in independent_functions:
                search_queries.append(f"{func_name} å‡½æ•° {language}")
                search_targets.append(f"æ–‡ä»¶-å‡½æ•°({func_name})")

            total_searches = len(search_queries)
            logger.info(f"ğŸ” å¼€å§‹ä¸º {file_path} æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œå…± {total_searches} ä¸ªæŸ¥è¯¢")

            # 3. æ‰§è¡Œæ£€ç´¢ï¼Œæ”¶é›†æ‰€æœ‰ç»“æœ
            all_results = []
            for i, (query, target) in enumerate(zip(search_queries, search_targets), 1):
                try:
                    logger.info(f"   [{i}/{total_searches}] æ£€ç´¢ {target}: {query}")
                    results = self.rag_client.search_knowledge(query=query, index_name=vectorstore_index, top_k=5)

                    found_count = 0
                    for result in results:
                        doc = result.get("document", {})
                        title = doc.get("title", "")
                        content_snippet = doc.get("content", "")
                        if title and content_snippet:
                            all_results.append(
                                {
                                    "title": title,
                                    "content": content_snippet,
                                    "file_path": doc.get("file_path", ""),
                                    "file": doc.get("file", ""),
                                    "category": doc.get("category", ""),
                                    "language": doc.get("language", ""),
                                    "query": query,
                                    "search_target": target,
                                }
                            )
                            found_count += 1

                    logger.info(f"       æ‰¾åˆ° {found_count} ä¸ªç›¸å…³ç»“æœ")

                except Exception as e:
                    logger.warning(f"   [{i}/{total_searches}] æ£€ç´¢å¤±è´¥ {target}: {str(e)}")
                    continue

            # 4. ç®€å•å»é‡å¹¶ç»„åˆç»“æœ
            seen_content = set()
            unique_results = []
            for result in all_results:
                content_hash = hash(result["content"])
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_results.append(result)

            # 5. ç»„åˆæ£€ç´¢ç»“æœ
            if unique_results:
                context_parts = ["=== RAG æ£€ç´¢ä¸Šä¸‹æ–‡ ==="]

                # æŒ‰æ£€ç´¢ç›®æ ‡åˆ†ç»„
                target_groups = {}
                for result in unique_results[:15]:  # é™åˆ¶æœ€å¤š15ä¸ªç»“æœ
                    target = result.get("search_target", "æœªçŸ¥ç›®æ ‡")
                    if target not in target_groups:
                        target_groups[target] = []
                    target_groups[target].append(result)

                # æŒ‰ç›®æ ‡åˆ†ç»„æ˜¾ç¤ºç»“æœ
                for target, results in target_groups.items():
                    context_parts.append(f"\n--- æ£€ç´¢ç›®æ ‡: {target} ---")
                    for i, result in enumerate(results[:3], 1):  # æ¯ä¸ªç›®æ ‡æœ€å¤š3ä¸ªç»“æœ
                        title = result.get("title", "Unknown")
                        content_snippet = result.get("content", "")
                        file_info = result.get("file", result.get("file_path", ""))
                        category = result.get("category", "")

                        # æˆªå–åˆé€‚é•¿åº¦
                        snippet = content_snippet[:400] + "..." if len(content_snippet) > 400 else content_snippet

                        context_parts.append(f"  {i}. {title} ({category})")
                        if file_info:
                            context_parts.append(f"     æ–‡ä»¶: {file_info}")
                        context_parts.append(f"     {snippet}\n")

                context = "\n".join(context_parts)
                logger.info(f"âœ… ä¸º {file_path} æ£€ç´¢åˆ° {len(unique_results)} ä¸ªç›¸å…³ä¸Šä¸‹æ–‡")
                return context
            else:
                logger.info(f"âš ï¸ æœªæ‰¾åˆ° {file_path} çš„ç›¸å…³ä¸Šä¸‹æ–‡")
                return ""

        except Exception as e:
            logger.warning(f"Failed to get RAG context for {file_item['file_path']}: {str(e)}")
            return ""

    def _extract_class_method_relationships(self, content: str, language: str) -> Dict[str, List[str]]:
        """æå–ç±»å’Œæ–¹æ³•çš„å…³è”å…³ç³»"""
        import re
        import ast

        relationships = {}

        if language == "python":
            try:
                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        class_name = node.name
                        methods = []

                        # æå–ç±»ä¸­çš„æ–¹æ³•
                        for item in node.body:
                            if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                                methods.append(item.name)

                        relationships[class_name] = methods

            except Exception as e:
                logger.warning(f"Failed to parse AST: {e}")
                # å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼
                return self._extract_class_method_relationships_regex(content)
        else:
            # å¯¹äºé Python è¯­è¨€ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
            return self._extract_class_method_relationships_regex(content)

        return relationships

    def _extract_independent_functions(
        self, content: str, language: str, class_relationships: Dict[str, List[str]]
    ) -> List[str]:
        """æå–ç‹¬ç«‹å‡½æ•°ï¼ˆä¸åœ¨ç±»ä¸­çš„å‡½æ•°ï¼‰"""
        import re
        import ast

        independent_functions = []

        if language == "python":
            try:
                tree = ast.parse(content)

                # æ”¶é›†æ‰€æœ‰ç±»ä¸­çš„æ–¹æ³•å
                class_methods = set()
                for methods in class_relationships.values():
                    class_methods.update(methods)

                # æŸ¥æ‰¾æ¨¡å—çº§åˆ«çš„å‡½æ•°ï¼ˆä¸åœ¨ç±»ä¸­çš„å‡½æ•°ï¼‰
                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        # æ£€æŸ¥å‡½æ•°æ˜¯å¦åœ¨æ¨¡å—çº§åˆ«ï¼ˆä¸åœ¨ç±»ä¸­ï¼‰
                        parent = getattr(node, "parent", None)
                        if parent is None or not isinstance(parent, ast.ClassDef):
                            # è¿™æ˜¯ä¸€ä¸ªæ¨¡å—çº§åˆ«çš„å‡½æ•°
                            func_name = node.name
                            if func_name not in class_methods and not func_name.startswith("_"):
                                independent_functions.append(func_name)

                # å¦‚æœASTè§£ææ²¡æœ‰parentä¿¡æ¯ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
                if not independent_functions:
                    return self._extract_independent_functions_regex(content, class_relationships)

            except Exception as e:
                logger.warning(f"Failed to parse AST for independent functions: {e}")
                return self._extract_independent_functions_regex(content, class_relationships)
        else:
            # å¯¹äºéPythonè¯­è¨€ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
            return self._extract_independent_functions_regex(content, class_relationships)

        # è¿‡æ»¤å’Œé™åˆ¶æ•°é‡
        filtered_functions = []
        common_words = {"main", "init", "test", "setup", "teardown", "__init__", "__main__"}

        for func_name in set(independent_functions):
            if len(func_name) > 2 and func_name.lower() not in common_words:
                filtered_functions.append(func_name)

        return filtered_functions[:10]  # é™åˆ¶æ•°é‡

    def _extract_independent_functions_regex(
        self, content: str, class_relationships: Dict[str, List[str]]
    ) -> List[str]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç‹¬ç«‹å‡½æ•°ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        import re

        independent_functions = []
        lines = content.split("\n")

        # æ”¶é›†æ‰€æœ‰ç±»ä¸­çš„æ–¹æ³•å
        class_methods = set()
        for methods in class_relationships.values():
            class_methods.update(methods)

        # æŸ¥æ‰¾å‡½æ•°å®šä¹‰ï¼Œæ’é™¤ç±»ä¸­çš„æ–¹æ³•
        in_class = False
        class_indent = 0

        for line in lines:
            stripped_line = line.strip()
            if not stripped_line:
                continue

            line_indent = len(line) - len(line.lstrip())

            # æ£€æµ‹ç±»å®šä¹‰
            if re.match(r"^class\s+", stripped_line):
                in_class = True
                class_indent = line_indent
                continue

            # æ£€æŸ¥æ˜¯å¦é€€å‡ºç±»
            if in_class and line_indent <= class_indent and stripped_line and not stripped_line.startswith("#"):
                if not re.match(r"^(def|async\s+def|@)", stripped_line):
                    in_class = False

            # æŸ¥æ‰¾å‡½æ•°å®šä¹‰
            func_match = re.match(r"^(async\s+)?def\s+([A-Za-z_][A-Za-z0-9_]*)", stripped_line)
            if func_match and not in_class:
                func_name = func_match.group(2)
                if func_name not in class_methods and not func_name.startswith("_"):
                    independent_functions.append(func_name)

        return list(set(independent_functions))

    def _extract_class_method_relationships_regex(self, content: str) -> Dict[str, List[str]]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç±»-æ–¹æ³•å…³ç³»ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        import re

        relationships = {}
        lines = content.split("\n")
        current_class = None
        indent_level = 0

        for line in lines:
            stripped_line = line.strip()
            if not stripped_line:
                continue

            # è®¡ç®—ç¼©è¿›çº§åˆ«
            line_indent = len(line) - len(line.lstrip())

            # æ£€æµ‹ç±»å®šä¹‰
            class_match = re.match(r"^class\s+([A-Za-z_][A-Za-z0-9_]*)", stripped_line)
            if class_match:
                current_class = class_match.group(1)
                relationships[current_class] = []
                indent_level = line_indent
                continue

            # æ£€æµ‹æ–¹æ³•å®šä¹‰ï¼ˆåœ¨ç±»å†…éƒ¨ï¼‰
            if current_class and line_indent > indent_level:
                method_match = re.match(r"^(async\s+)?def\s+([A-Za-z_][A-Za-z0-9_]*)", stripped_line)
                if method_match:
                    method_name = method_match.group(2)
                    # è¿‡æ»¤æ‰ä¸€äº›æ— æ„ä¹‰çš„æ–¹æ³•å
                    if method_name not in {"__str__", "__repr__", "__eq__", "__hash__"}:
                        relationships[current_class].append(method_name)
            elif current_class and line_indent <= indent_level:
                # é€€å‡ºå½“å‰ç±»
                current_class = None

        return relationships

    async def post_async(
        self, shared: Dict[str, Any], prep_res: List[Dict[str, Any]], exec_res: List[Dict[str, Any]]
    ) -> str:
        """
        æ•´ç†æ‰€æœ‰ç»“æœï¼Œæ›´æ–°å…±äº«çŠ¶æ€

        Data Access:
        - Write: shared.web_code_analysis
        """
        # è¿‡æ»¤æ‰é”™è¯¯ç»“æœ
        valid_results = [r for r in exec_res if not r.get("error")]
        error_count = len(exec_res) - len(valid_results)

        shared["web_code_analysis"] = valid_results

        # ç»Ÿè®¡ä¿¡æ¯
        total_analysis_items = sum(len(r.get("analysis_items", [])) for r in valid_results)
        total_functions = sum(len(r.get("functions", [])) for r in valid_results)
        total_classes = sum(len(r.get("classes", [])) for r in valid_results)

        logger.info(
            f"Web code analysis completed: {len(valid_results)} files, "
            f"{total_functions} functions, {total_classes} classes, "
            f"{total_analysis_items} analysis items, {error_count} errors"
        )

        return "default"

    async def exec_fallback_async(self, file_item: Dict[str, Any], exc: Exception) -> Dict[str, Any]:
        """
        è‹¥è§£æå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        """
        logger.warning(f"Fallback parsing for {file_item['file_path']}: {str(exc)}")

        return {
            "file_analysis_id": file_item["file_analysis_id"],
            "file_path": file_item["file_path"],
            "analysis_items": [],
            "error": f"Fallback: {str(exc)}",
        }
