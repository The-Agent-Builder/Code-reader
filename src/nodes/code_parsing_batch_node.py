"""
CodeParsingBatchNode - å¹¶è¡Œè§£ææ‰€æœ‰æºç æ–‡ä»¶ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯
Design: AsyncParallelBatchNode, batch_size=10, max_retries=2, wait=20
"""

from typing import Dict, Any, List
from pathlib import Path
from pocketflow import AsyncParallelBatchNode

from ..utils.llm_parser import LLMParser
from ..utils.rag_api_client import RAGAPIClient
from ..utils.logger import logger
from ..utils.error_handler import LLMParsingError
from ..utils.config import get_config


class CodeParsingBatchNode(AsyncParallelBatchNode):
    """å¹¶è¡Œè§£ææ‰€æœ‰æºç æ–‡ä»¶ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯èŠ‚ç‚¹"""

    def __init__(self, batch_size: int = None):
        super().__init__(max_retries=2, wait=20)
        self.llm_parser = LLMParser()  # LLMè§£æå™¨

        config = get_config()
        self.rag_client = RAGAPIClient(config.rag_base_url)  # RAG APIå®¢æˆ·ç«¯
        self.batch_size = batch_size if batch_size is not None else config.llm_batch_size
        self.max_concurrent = config.llm_max_concurrent

    async def prep_async(self, shared: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä¸ºæ¯ä¸ªæ–‡ä»¶å‡†å¤‡ç‹¬ç«‹çš„ä¸Šä¸‹æ–‡å‚æ•°

        Data Access:
        - Read: shared.vectorstore_index, shared.local_path
        """
        # æ ¹æ®æ˜¯å¦æœ‰å‘é‡åŒ–é˜¶æ®µæ¥ç¡®å®šå½“å‰æ˜¯ç¬¬å‡ é˜¶æ®µ
        logger.info("=" * 60)
        if shared.get("current_stage") == "vectorization":
            logger.info("ğŸ“‹ é˜¶æ®µ 4/4: ä»£ç åˆ†æ (CodeParsingBatchNode)")
        else:
            logger.info("ğŸ“‹ é˜¶æ®µ 3/3: ä»£ç åˆ†æ (CodeParsingBatchNode)")

        shared["current_stage"] = "code_analysis"

        local_path = shared.get("local_path")
        vectorstore_index = shared.get("vectorstore_index")

        if not local_path:
            logger.error("âŒ ä»£ç åˆ†æéœ€è¦æä¾›æœ¬åœ°ä»“åº“è·¯å¾„")
            raise LLMParsingError("Local path is required")

        local_path = Path(local_path)
        logger.info(f"ğŸ” æ‰«ææºç æ–‡ä»¶: {local_path}")

        # æ”¶é›†æ‰€æœ‰éœ€è¦è§£æçš„ä»£ç æ–‡ä»¶
        file_items = []
        supported_extensions = {
            ".py",
            ".js",
            ".ts",
            ".java",
            ".cpp",
            ".c",
            ".h",
            ".hpp",
            ".cs",
            ".go",
            ".rs",
            ".php",
            ".rb",
            ".swift",
            ".kt",
            ".scala",
            ".ipynb",
        }

        ignore_dirs = {".git", "__pycache__", "node_modules", ".venv", "venv", "env"}

        for file_path in local_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                # è·³è¿‡å¿½ç•¥çš„ç›®å½•
                if any(ignore_dir in file_path.parts for ignore_dir in ignore_dirs):
                    continue

                try:
                    if file_path.suffix.lower() == ".ipynb":
                        content = self._extract_notebook_content(file_path)
                        if not content:
                            continue
                    else:
                        with open(file_path, "r", encoding="utf-8") as f:
                            content = f.read()

                    # è·³è¿‡ç©ºæ–‡ä»¶æˆ–è¿‡å¤§çš„æ–‡ä»¶
                    # if len(content.strip()) == 0 or len(content) > 50000:
                    #     continue

                    # ç¡®å®šè¯­è¨€
                    language_map = {
                        ".py": "python",
                        ".js": "javascript",
                        ".ts": "typescript",
                        ".java": "java",
                        ".cpp": "cpp",
                        ".c": "c",
                        ".h": "c",
                        ".hpp": "cpp",
                        ".cs": "csharp",
                        ".go": "go",
                        ".rs": "rust",
                        ".php": "php",
                        ".rb": "ruby",
                        ".swift": "swift",
                        ".kt": "kotlin",
                        ".scala": "scala",
                        ".ipynb": "jupyter",
                    }
                    language = language_map.get(file_path.suffix.lower(), "text")

                    file_items.append(
                        {
                            "file_path": str(file_path.relative_to(local_path)),
                            "content": content,
                            "language": language,
                            "full_path": str(file_path),
                            "vectorstore_index": vectorstore_index,
                        }
                    )

                except Exception as e:
                    logger.warning(f"Failed to read file {file_path}: {str(e)}")
                    continue

        logger.info(f"å‡†å¤‡è§£æ {len(file_items)} ä¸ªæ–‡ä»¶")

        # åˆå§‹åŒ–å®æ—¶å†™å…¥çš„ markdown æ–‡ä»¶
        await self._initialize_analysis_file(shared)

        # æ·»åŠ è¿›åº¦å›è°ƒå’Œç´¢å¼•ä¿¡æ¯
        progress_callback = shared.get("progress_callback")
        for i, file_item in enumerate(file_items):
            file_item["progress_callback"] = progress_callback
            file_item["current_index"] = i
            # æ·»åŠ å…±äº«æ•°æ®å¼•ç”¨ï¼Œç”¨äºå®æ—¶å†™å…¥
            file_item["shared"] = shared

        return file_items

    async def exec_async(self, file_item: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨ RAG API è·å–ä¸Šä¸‹æ–‡ï¼Œç„¶åè°ƒç”¨ LLM åˆ†ææ–‡ä»¶å†…å®¹ï¼Œç”Ÿæˆè¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£
        """
        try:
            # 1. ä½¿ç”¨ RAG API è·å–ç›¸å…³ä¸Šä¸‹æ–‡
            context = await self._get_rag_context(file_item)

            # 2. è°ƒç”¨ LLM è¿›è¡Œè¯¦ç»†åˆ†æï¼Œç”Ÿæˆç±»ä¼¼ res.md æ ¼å¼çš„å†…å®¹
            result = await self.llm_parser.parse_code_file_detailed(
                file_item["file_path"], file_item["content"], file_item["language"], context
            )

            # 3. ç«‹å³å†™å…¥åˆ†æç»“æœåˆ° markdown æ–‡ä»¶
            if not result.get("error"):
                await self._append_analysis_to_file(result, file_item.get("shared", {}))

            # 4. è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            progress_callback = file_item.get("progress_callback")
            if progress_callback:
                # ä½¿ç”¨é€’å¢æ¨¡å¼ï¼Œä¸ä¼ å…¥completedå‚æ•°ï¼Œè®©å›è°ƒå‡½æ•°è‡ªå·±è®¡æ•°
                current_file = file_item["file_path"]
                try:
                    progress_callback(current_file=current_file)
                except Exception as e:
                    logger.warning(f"Progress callback failed: {str(e)}")

            return result

        except Exception as e:
            logger.error(f"Failed to parse file {file_item['file_path']}: {str(e)}")
            # è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œä¿æŒæ‰¹å¤„ç†ç»§ç»­
            return {"file_path": file_item["file_path"], "analysis_items": [], "error": str(e)}

    async def _get_rag_context(self, file_item: Dict[str, Any]) -> str:
        """
        ä½¿ç”¨ RAG API è·å–ä¸å½“å‰æ–‡ä»¶ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        ç®€åŒ–ç‰ˆï¼šæŒ‰ç±»ã€æ–¹æ³•æˆ–æ•´ä¸ªæ–‡ä»¶è¿›è¡Œæ£€ç´¢ï¼Œä¸é‡æ–°æ’åºï¼Œç›´æ¥é€šè¿‡RAG LLMå¢å¼º
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘é‡å­˜å‚¨ç´¢å¼•
            vectorstore_index = file_item.get("vectorstore_index")
            if not vectorstore_index:
                logger.warning("RAG API ç´¢å¼•ä¸å¯ç”¨")
                return ""

            file_path = file_item["file_path"]
            content = file_item["content"]
            language = file_item["language"]

            # 1. æå–ç±»-æ–¹æ³•å…³è”å…³ç³»å’Œç‹¬ç«‹å‡½æ•°
            class_method_relationships = self._extract_class_method_relationships(content, language)
            independent_functions = self._extract_independent_functions(content, language, class_method_relationships)

            logger.info(f"ğŸ” ä» {file_path} æ–‡ä»¶ä¸­æå–åˆ°:")
            logger.info(f"   - ç±»: {list(class_method_relationships.keys())}")
            logger.info(f"   - ç‹¬ç«‹å‡½æ•°: {independent_functions}")

            # 2. æ„å»ºç»†ç²’åº¦æ£€ç´¢ç­–ç•¥ï¼šæ–‡ä»¶ + æ¯ä¸ªç±» + æ¯ä¸ªç‹¬ç«‹å‡½æ•°
            search_queries = []
            search_targets = []  # ç”¨äºè®°å½•æ£€ç´¢ç›®æ ‡

            # 2.1 å¯¹æ–‡ä»¶æœ¬èº«è¿›è¡Œæ£€ç´¢
            search_queries.append(f"{file_path} {language} æ–‡ä»¶")
            search_targets.append(f"æ–‡ä»¶-{file_path}")

            # 2.2 å¯¹æ¯ä¸ªç±»åˆ†åˆ«è¿›è¡Œæ£€ç´¢
            for class_name in class_method_relationships.keys():
                search_queries.append(f"{class_name} ç±» {language}")
                search_targets.append(f"æ–‡ä»¶-ç±»({class_name})")

            # 2.3 å¯¹æ¯ä¸ªç‹¬ç«‹å‡½æ•°åˆ†åˆ«è¿›è¡Œæ£€ç´¢
            for func_name in independent_functions:
                search_queries.append(f"{func_name} å‡½æ•° {language}")
                search_targets.append(f"æ–‡ä»¶-å‡½æ•°({func_name})")

            total_searches = len(search_queries)
            logger.info(f"ğŸ” å¼€å§‹ä¸º {file_path} æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œå…± {total_searches} ä¸ªæŸ¥è¯¢:")
            logger.info(f"   - æ–‡ä»¶æ£€ç´¢: 1æ¬¡")
            logger.info(f"   - ç±»æ£€ç´¢: {len(class_method_relationships)}æ¬¡")
            logger.info(f"   - ç‹¬ç«‹å‡½æ•°æ£€ç´¢: {len(independent_functions)}æ¬¡")

            # 3. æ‰§è¡Œæ£€ç´¢ï¼Œæ”¶é›†æ‰€æœ‰ç»“æœ
            all_results = []
            for i, (query, target) in enumerate(zip(search_queries, search_targets), 1):
                try:
                    logger.info(f"   [{i}/{total_searches}] æ£€ç´¢ {target}: {query}")
                    results = self.rag_client.search_knowledge(query=query, index_name=vectorstore_index, top_k=5)

                    found_count = 0
                    for result in results:
                        doc = result.get("document", {})
                        title = doc.get("title", "")
                        content_snippet = doc.get("content", "")
                        if title and content_snippet:
                            all_results.append(
                                {
                                    "title": title,
                                    "content": content_snippet,
                                    "file_path": doc.get("file_path", ""),
                                    "file": doc.get("file", ""),
                                    "category": doc.get("category", ""),
                                    "language": doc.get("language", ""),
                                    "query": query,
                                    "search_target": target,  # æ·»åŠ æ£€ç´¢ç›®æ ‡ä¿¡æ¯
                                }
                            )
                            found_count += 1

                    logger.info(f"       æ‰¾åˆ° {found_count} ä¸ªç›¸å…³ç»“æœ")

                except Exception as e:
                    logger.warning(f"   [{i}/{total_searches}] æ£€ç´¢å¤±è´¥ {target}: {str(e)}")
                    continue

            # 4. ç®€å•å»é‡ï¼ˆåŸºäºå†…å®¹ï¼‰
            seen_content = set()
            unique_results = []
            for result in all_results:
                content_hash = hash(result["content"])
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_results.append(result)

            # 5. ç›´æ¥ç»„åˆæ‰€æœ‰æ£€ç´¢ç»“æœï¼ŒæŒ‰æ£€ç´¢ç›®æ ‡åˆ†ç»„æ˜¾ç¤º
            if unique_results:
                context_parts = ["=== RAG æ£€ç´¢ä¸Šä¸‹æ–‡ ==="]

                # æŒ‰æ£€ç´¢ç›®æ ‡åˆ†ç»„
                target_groups = {}
                for result in unique_results[:15]:  # é™åˆ¶æœ€å¤š15ä¸ªç»“æœ
                    target = result.get("search_target", "æœªçŸ¥ç›®æ ‡")
                    if target not in target_groups:
                        target_groups[target] = []
                    target_groups[target].append(result)

                # æŒ‰ç›®æ ‡åˆ†ç»„æ˜¾ç¤ºç»“æœ
                for target, results in target_groups.items():
                    context_parts.append(f"\n--- æ£€ç´¢ç›®æ ‡: {target} ---")
                    for i, result in enumerate(results[:3], 1):  # æ¯ä¸ªç›®æ ‡æœ€å¤š3ä¸ªç»“æœ
                        title = result.get("title", "Unknown")
                        content_snippet = result.get("content", "")
                        file_info = result.get("file", result.get("file_path", ""))
                        category = result.get("category", "")

                        # æˆªå–åˆé€‚é•¿åº¦
                        snippet = content_snippet[:400] + "..." if len(content_snippet) > 400 else content_snippet

                        context_parts.append(f"  {i}. {title} ({category})")
                        if file_info:
                            context_parts.append(f"     æ–‡ä»¶: {file_info}")
                        context_parts.append(f"     {snippet}\n")

                context = "\n".join(context_parts)
                logger.info(
                    f"âœ… ä¸º {file_path} æ£€ç´¢åˆ° {len(unique_results)} ä¸ªç›¸å…³ä¸Šä¸‹æ–‡ï¼Œåˆ†å¸ƒåœ¨ {len(target_groups)} ä¸ªæ£€ç´¢ç›®æ ‡ä¸­"
                )
                return context
            else:
                logger.info(f"âš ï¸ æœªæ‰¾åˆ° {file_path} çš„ç›¸å…³ä¸Šä¸‹æ–‡")
                return ""

        except Exception as e:
            logger.warning(f"Failed to get RAG context for {file_item['file_path']}: {str(e)}")
            return ""

    def _extract_class_method_relationships(self, content: str, language: str) -> Dict[str, List[str]]:
        """æå–ç±»å’Œæ–¹æ³•çš„å…³è”å…³ç³»"""
        import re
        import ast

        relationships = {}

        if language == "python":
            try:
                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        class_name = node.name
                        methods = []

                        # æå–ç±»ä¸­çš„æ–¹æ³•
                        for item in node.body:
                            if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                                methods.append(item.name)

                        relationships[class_name] = methods

            except Exception as e:
                logger.warning(f"Failed to parse AST: {e}")
                # å›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼
                return self._extract_class_method_relationships_regex(content)
        else:
            # å¯¹äºé Python è¯­è¨€ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
            return self._extract_class_method_relationships_regex(content)

        return relationships

    async def _initialize_analysis_file(self, shared: Dict[str, Any]):
        """åˆå§‹åŒ–å®æ—¶åˆ†ææ–‡ä»¶ï¼ˆmarkdown å’Œ JSONï¼‰"""
        try:
            # è·å–ä»“åº“ä¿¡æ¯
            repo_info = shared.get("repo_info", {})
            repo_name = repo_info.get("name", "unknown")

            # åˆ›å»ºä»“åº“ä¸“ç”¨çš„ç»“æœç›®å½•: data/results/ä»“åº“å/
            repo_results_dir = Path("./data/results") / repo_name
            repo_results_dir.mkdir(parents=True, exist_ok=True)

            # ç”Ÿæˆåˆ†ææŠ¥å‘Šæ–‡ä»¶å
            doc_path = repo_results_dir / "analysis_report.md"
            json_path = repo_results_dir / "analysis_report.json"

            # åˆå§‹åŒ– markdown æ–‡ä»¶ï¼ˆæ¸…ç©ºæˆ–åˆ›å»ºï¼‰
            with open(doc_path, "w", encoding="utf-8") as f:
                f.write("# ä»£ç åˆ†ææŠ¥å‘Š\n\n")
                f.write(f"ä»“åº“: {repo_name}\n")
                f.write(f"åˆ†ææ—¶é—´: {self._get_current_time()}\n\n")
                f.write("---\n\n")

            # åˆå§‹åŒ– JSON æ–‡ä»¶
            import json

            initial_json_data = {
                "repository": {"name": repo_name, "info": repo_info, "analysis_time": self._get_current_time()},
                "files": [],
                "statistics": {"total_files": 0, "total_functions": 0, "total_classes": 0, "total_snippets": 0},
            }

            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(initial_json_data, f, ensure_ascii=False, indent=2)

            # ä¿å­˜æ–‡ä»¶è·¯å¾„åˆ°å…±äº«æ•°æ®
            shared["analysis_report_path"] = str(doc_path)
            shared["analysis_json_path"] = str(json_path)
            logger.info(f"ğŸ“„ åˆå§‹åŒ–åˆ†ææŠ¥å‘Šæ–‡ä»¶: {doc_path}")
            logger.info(f"ğŸ“„ åˆå§‹åŒ– JSON æ•°æ®æ–‡ä»¶: {json_path}")

        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–åˆ†ææ–‡ä»¶å¤±è´¥: {str(e)}")

    async def _append_analysis_to_file(self, result: Dict[str, Any], shared: Dict[str, Any]):
        """å°†å•ä¸ªæ–‡ä»¶çš„åˆ†æç»“æœè¿½åŠ åˆ° markdown å’Œ JSON æ–‡ä»¶"""
        try:
            analysis_file_path = shared.get("analysis_report_path")
            json_file_path = shared.get("analysis_json_path")

            if not analysis_file_path or not json_file_path:
                logger.warning("åˆ†ææŠ¥å‘Šæ–‡ä»¶è·¯å¾„æœªæ‰¾åˆ°ï¼Œè·³è¿‡å†™å…¥")
                return

            file_path = result["file_path"]
            items = result.get("analysis_items", [])

            if not items:
                return

            # ä¸ºæ¯ä¸ªåˆ†æé¡¹æ·»åŠ æ–‡ä»¶è·¯å¾„å­—æ®µå’Œæ£€ç´¢ç›®æ ‡ä¿¡æ¯
            enhanced_items = []
            for item in items:
                enhanced_item = item.copy()
                enhanced_item["file_path"] = file_path  # æ·»åŠ æ–‡ä»¶è·¯å¾„å­—æ®µ

                # å°è¯•ä»åˆ†æé¡¹ä¸­æå–æ£€ç´¢ç›®æ ‡ä¿¡æ¯
                title = item.get("title", "")
                source = item.get("source", "")

                # æ ¹æ®åˆ†æé¡¹çš„ç‰¹å¾æ¨æ–­æ£€ç´¢ç›®æ ‡ç±»å‹
                search_target = self._infer_search_target(title, source, file_path)
                enhanced_item["search_target"] = search_target

                enhanced_items.append(enhanced_item)

            # 1. å†™å…¥åˆ° markdown æ–‡ä»¶
            await self._append_to_markdown(file_path, enhanced_items, analysis_file_path, shared)

            # 2. å†™å…¥åˆ° JSON æ–‡ä»¶
            await self._append_to_json(file_path, enhanced_items, json_file_path, result)

            logger.info(f"âœ… å·²å°† {file_path} çš„åˆ†æç»“æœå†™å…¥ markdown å’Œ JSON æ–‡ä»¶")

        except Exception as e:
            logger.error(f"âŒ è¿½åŠ åˆ†æç»“æœåˆ°æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _infer_search_target(self, title: str, source: str, file_path: str) -> str:
        """æ ¹æ®åˆ†æé¡¹çš„ç‰¹å¾æ¨æ–­æ£€ç´¢ç›®æ ‡ç±»å‹"""
        title_lower = title.lower()

        # æ£€æŸ¥æ˜¯å¦æ˜¯ç±»
        if "class" in title_lower or "ç±»" in title:
            # å°è¯•æå–ç±»å
            import re

            class_match = re.search(r"class\s+([A-Za-z_][A-Za-z0-9_]*)", title, re.IGNORECASE)
            if class_match:
                class_name = class_match.group(1)
                return f"æ–‡ä»¶-ç±»({class_name})"
            else:
                return f"æ–‡ä»¶-ç±»(æœªçŸ¥)"

        # æ£€æŸ¥æ˜¯å¦æ˜¯å‡½æ•°/æ–¹æ³•
        elif any(keyword in title_lower for keyword in ["function", "method", "def", "å‡½æ•°", "æ–¹æ³•"]):
            # å°è¯•æå–å‡½æ•°å
            import re

            func_patterns = [
                r"def\s+([A-Za-z_][A-Za-z0-9_]*)",
                r"function\s+([A-Za-z_][A-Za-z0-9_]*)",
                r"([A-Za-z_][A-Za-z0-9_]*)\s*\(",
                r"æ–¹æ³•\s*([A-Za-z_][A-Za-z0-9_]*)",
                r"å‡½æ•°\s*([A-Za-z_][A-Za-z0-9_]*)",
            ]

            for pattern in func_patterns:
                func_match = re.search(pattern, title, re.IGNORECASE)
                if func_match:
                    func_name = func_match.group(1)
                    return f"æ–‡ä»¶-å‡½æ•°({func_name})"

            return f"æ–‡ä»¶-å‡½æ•°(æœªçŸ¥)"

        # é»˜è®¤ä¸ºæ–‡ä»¶çº§åˆ«
        return f"æ–‡ä»¶-{file_path}"

    async def _append_to_markdown(
        self, file_path: str, items: List[Dict[str, Any]], markdown_path: str, shared: Dict[str, Any]
    ):
        """è¿½åŠ åˆ†æç»“æœåˆ° markdown æ–‡ä»¶ï¼ŒæŒ‰æ£€ç´¢ç›®æ ‡åˆ†ç»„"""
        try:
            # æŒ‰æ£€ç´¢ç›®æ ‡åˆ†ç»„
            target_groups = {}
            for item in items:
                target = item.get("search_target", f"æ–‡ä»¶-{file_path}")
                if target not in target_groups:
                    target_groups[target] = []
                target_groups[target].append(item)

            # è¿½åŠ å†™å…¥åˆ° markdown æ–‡ä»¶
            with open(markdown_path, "a", encoding="utf-8") as f:
                f.write(f"## æ–‡ä»¶: {file_path}\n\n")

                # æŒ‰æ£€ç´¢ç›®æ ‡åˆ†ç»„æ˜¾ç¤º
                for target, target_items in target_groups.items():
                    f.write(f"### æ£€ç´¢ç›®æ ‡: {target}\n\n")

                    for item in target_items:
                        title = item.get("title", "Unknown")
                        description = item.get("description", "No description")
                        source = item.get("source", "Unknown source")
                        language = item.get("language", "unknown")
                        code = item.get("code", "")

                        # æ„å»º SOURCE é“¾æ¥ï¼ˆå¦‚æœæœ‰ä»“åº“URLï¼‰
                        repo_url = shared.get("repo_url", "")
                        if repo_url and source:
                            # ä» source ä¸­æå–æ–‡ä»¶è·¯å¾„å’Œè¡Œå·
                            if ":" in source:
                                file_part, line_part = source.split(":", 1)
                                if "-" in line_part:
                                    start_line, end_line = line_part.split("-", 1)
                                    source_url = f"{repo_url}/blob/main/{file_part}#L{start_line}-L{end_line}"
                                else:
                                    source_url = f"{repo_url}/blob/main/{file_part}#L{line_part}"
                            else:
                                source_url = f"{repo_url}/blob/main/{source}"
                        else:
                            source_url = source

                        # æŒ‰ç…§ res.md çš„ç²¾ç¡®æ ¼å¼ç”Ÿæˆæ¡ç›®
                        item_content = f"""TITLE: {title}
DESCRIPTION: {description}
SOURCE: {source_url}
SEARCH_TARGET: {target}

LANGUAGE: {language}
CODE:
```
{code}
```

----------------------------------------

"""
                        f.write(item_content)

                    f.write("\n")  # æ¯ä¸ªæ£€ç´¢ç›®æ ‡åæ·»åŠ ç©ºè¡Œ

                f.write("\n")  # æ–‡ä»¶ç»“æŸåæ·»åŠ ç©ºè¡Œ

        except Exception as e:
            logger.error(f"âŒ å†™å…¥ markdown æ–‡ä»¶å¤±è´¥: {str(e)}")

    async def _append_to_json(
        self, file_path: str, items: List[Dict[str, Any]], json_path: str, result: Dict[str, Any]
    ):
        """è¿½åŠ åˆ†æç»“æœåˆ° JSON æ–‡ä»¶ï¼ŒåŒ…å«æ£€ç´¢ç›®æ ‡ä¿¡æ¯"""
        try:
            import json

            # è¯»å–ç°æœ‰çš„ JSON æ•°æ®
            with open(json_path, "r", encoding="utf-8") as f:
                json_data = json.load(f)

            # æŒ‰æ£€ç´¢ç›®æ ‡åˆ†ç»„åˆ†æé¡¹
            target_groups = {}
            for item in items:
                target = item.get("search_target", f"æ–‡ä»¶-{file_path}")
                if target not in target_groups:
                    target_groups[target] = []
                target_groups[target].append(item)

            # ä»åˆ†æé¡¹ä¸­æ¨æ–­æ–‡ä»¶è¯­è¨€ï¼ˆå¦‚æœresultä¸­æ²¡æœ‰è¯­è¨€ä¿¡æ¯ï¼‰
            file_language = result.get("language", "unknown")
            if file_language == "unknown" and items:
                # å°è¯•ä»ç¬¬ä¸€ä¸ªåˆ†æé¡¹ä¸­è·å–è¯­è¨€ä¿¡æ¯
                first_item_language = items[0].get("language", "unknown")
                if first_item_language != "unknown":
                    file_language = first_item_language

            # æ„å»ºæ–‡ä»¶åˆ†ææ•°æ®
            file_data = {
                "file_path": file_path,
                "language": file_language,
                "analysis_items": items,  # ä¿ç•™åŸå§‹åˆ†æé¡¹
                "analysis_items_by_target": target_groups,  # æŒ‰æ£€ç´¢ç›®æ ‡åˆ†ç»„çš„åˆ†æé¡¹
                "search_targets": list(target_groups.keys()),  # æ£€ç´¢ç›®æ ‡åˆ—è¡¨
                "functions": result.get("functions", []),
                "classes": result.get("classes", []),
                "code_snippets": result.get("code_snippets", []),
                "analysis_timestamp": self._get_current_time(),
                "target_statistics": {
                    "total_targets": len(target_groups),
                    "targets_detail": {target: len(target_items) for target, target_items in target_groups.items()},
                },
            }

            # æ·»åŠ åˆ°æ–‡ä»¶åˆ—è¡¨
            json_data["files"].append(file_data)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            json_data["statistics"]["total_files"] = len(json_data["files"])
            json_data["statistics"]["total_functions"] += len(result.get("functions", []))
            json_data["statistics"]["total_classes"] += len(result.get("classes", []))
            json_data["statistics"]["total_snippets"] += len(items)

            # æ›´æ–°æ£€ç´¢ç›®æ ‡ç»Ÿè®¡
            if "search_targets" not in json_data["statistics"]:
                json_data["statistics"]["search_targets"] = {}

            for target in target_groups.keys():
                if target not in json_data["statistics"]["search_targets"]:
                    json_data["statistics"]["search_targets"][target] = 0
                json_data["statistics"]["search_targets"][target] += len(target_groups[target])

            # å†™å› JSON æ–‡ä»¶
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"âŒ å†™å…¥ JSON æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _extract_function_names(self, content: str, language: str) -> List[str]:
        """æå–æ–‡ä»¶ä¸­çš„å‡½æ•°åï¼ˆç”¨äºæ²¡æœ‰ç±»çš„æƒ…å†µï¼‰"""
        import re

        function_names = []

        if language == "python":
            # æå– Python å‡½æ•°å
            func_pattern = r"^def\s+([A-Za-z_][A-Za-z0-9_]*)"
            matches = re.findall(func_pattern, content, re.MULTILINE)
            function_names.extend(matches)
        elif language in ["javascript", "typescript"]:
            # æå– JS/TS å‡½æ•°å
            func_patterns = [
                r"function\s+([A-Za-z_][A-Za-z0-9_]*)",
                r"const\s+([A-Za-z_][A-Za-z0-9_]*)\s*=\s*(?:async\s+)?(?:function|\()",
                r"([A-Za-z_][A-Za-z0-9_]*)\s*:\s*(?:async\s+)?function",
            ]
            for pattern in func_patterns:
                matches = re.findall(pattern, content)
                function_names.extend(matches)
        elif language == "java":
            # æå– Java æ–¹æ³•å
            method_pattern = r"(?:public|private|protected)?\s*(?:static)?\s*\w+\s+([A-Za-z_][A-Za-z0-9_]*)\s*\("
            matches = re.findall(method_pattern, content)
            function_names.extend(matches)

        # è¿‡æ»¤å¸¸è§çš„æ— æ„ä¹‰å‡½æ•°å
        filtered_names = []
        common_words = {
            "main",
            "init",
            "test",
            "get",
            "set",
            "new",
            "create",
            "update",
            "delete",
            "__init__",
            "__str__",
            "__repr__",
        }

        for name in set(function_names):
            if len(name) > 2 and name.lower() not in common_words:
                filtered_names.append(name)

        return filtered_names[:10]  # é™åˆ¶æ•°é‡

    def _extract_independent_functions(
        self, content: str, language: str, class_relationships: Dict[str, List[str]]
    ) -> List[str]:
        """æå–ç‹¬ç«‹å‡½æ•°ï¼ˆä¸åœ¨ç±»ä¸­çš„å‡½æ•°ï¼‰"""
        import re
        import ast

        independent_functions = []

        if language == "python":
            try:
                tree = ast.parse(content)

                # æ”¶é›†æ‰€æœ‰ç±»ä¸­çš„æ–¹æ³•å
                class_methods = set()
                for methods in class_relationships.values():
                    class_methods.update(methods)

                # æŸ¥æ‰¾æ¨¡å—çº§åˆ«çš„å‡½æ•°ï¼ˆä¸åœ¨ç±»ä¸­çš„å‡½æ•°ï¼‰
                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        # æ£€æŸ¥å‡½æ•°æ˜¯å¦åœ¨æ¨¡å—çº§åˆ«ï¼ˆä¸åœ¨ç±»ä¸­ï¼‰
                        parent = getattr(node, "parent", None)
                        if parent is None or not isinstance(parent, ast.ClassDef):
                            # è¿™æ˜¯ä¸€ä¸ªæ¨¡å—çº§åˆ«çš„å‡½æ•°
                            func_name = node.name
                            if func_name not in class_methods and not func_name.startswith("_"):
                                independent_functions.append(func_name)

                # å¦‚æœASTè§£ææ²¡æœ‰parentä¿¡æ¯ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
                if not independent_functions:
                    return self._extract_independent_functions_regex(content, class_relationships)

            except Exception as e:
                logger.warning(f"Failed to parse AST for independent functions: {e}")
                return self._extract_independent_functions_regex(content, class_relationships)
        else:
            # å¯¹äºéPythonè¯­è¨€ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
            return self._extract_independent_functions_regex(content, class_relationships)

        # è¿‡æ»¤å’Œé™åˆ¶æ•°é‡
        filtered_functions = []
        common_words = {"main", "init", "test", "setup", "teardown", "__init__", "__main__"}

        for func_name in set(independent_functions):
            if len(func_name) > 2 and func_name.lower() not in common_words:
                filtered_functions.append(func_name)

        return filtered_functions[:10]  # é™åˆ¶æ•°é‡

    def _extract_independent_functions_regex(
        self, content: str, class_relationships: Dict[str, List[str]]
    ) -> List[str]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç‹¬ç«‹å‡½æ•°ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        import re

        independent_functions = []
        lines = content.split("\n")

        # æ”¶é›†æ‰€æœ‰ç±»ä¸­çš„æ–¹æ³•å
        class_methods = set()
        for methods in class_relationships.values():
            class_methods.update(methods)

        # æŸ¥æ‰¾å‡½æ•°å®šä¹‰ï¼Œæ’é™¤ç±»ä¸­çš„æ–¹æ³•
        in_class = False
        class_indent = 0

        for line in lines:
            stripped_line = line.strip()
            if not stripped_line:
                continue

            line_indent = len(line) - len(line.lstrip())

            # æ£€æµ‹ç±»å®šä¹‰
            if re.match(r"^class\s+", stripped_line):
                in_class = True
                class_indent = line_indent
                continue

            # æ£€æŸ¥æ˜¯å¦é€€å‡ºç±»
            if in_class and line_indent <= class_indent and stripped_line and not stripped_line.startswith("#"):
                if not re.match(r"^(def|async\s+def|@)", stripped_line):
                    in_class = False

            # æŸ¥æ‰¾å‡½æ•°å®šä¹‰
            func_match = re.match(r"^(async\s+)?def\s+([A-Za-z_][A-Za-z0-9_]*)", stripped_line)
            if func_match and not in_class:
                func_name = func_match.group(2)
                if func_name not in class_methods and not func_name.startswith("_"):
                    independent_functions.append(func_name)

        return list(set(independent_functions))

    def _extract_class_method_relationships_regex(self, content: str) -> Dict[str, List[str]]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç±»-æ–¹æ³•å…³ç³»ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        import re

        relationships = {}
        lines = content.split("\n")
        current_class = None
        indent_level = 0

        for line in lines:
            stripped_line = line.strip()
            if not stripped_line:
                continue

            # è®¡ç®—ç¼©è¿›çº§åˆ«
            line_indent = len(line) - len(line.lstrip())

            # æ£€æµ‹ç±»å®šä¹‰
            class_match = re.match(r"^class\s+([A-Za-z_][A-Za-z0-9_]*)", stripped_line)
            if class_match:
                current_class = class_match.group(1)
                relationships[current_class] = []
                indent_level = line_indent
                continue

            # æ£€æµ‹æ–¹æ³•å®šä¹‰ï¼ˆåœ¨ç±»å†…éƒ¨ï¼‰
            if current_class and line_indent > indent_level:
                method_match = re.match(r"^(async\s+)?def\s+([A-Za-z_][A-Za-z0-9_]*)", stripped_line)
                if method_match:
                    method_name = method_match.group(2)
                    # è¿‡æ»¤æ‰ä¸€äº›æ— æ„ä¹‰çš„æ–¹æ³•å
                    if method_name not in {"__str__", "__repr__", "__eq__", "__hash__"}:
                        relationships[current_class].append(method_name)
            elif current_class and line_indent <= indent_level:
                # é€€å‡ºå½“å‰ç±»
                current_class = None

        return relationships

    def _detect_language(self, file_extension: str) -> str:
        """
        æ ¹æ®æ–‡ä»¶æ‰©å±•åæ£€æµ‹ç¼–ç¨‹è¯­è¨€
        """
        extension_map = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".java": "java",
            ".cpp": "cpp",
            ".c": "c",
            ".h": "c",
            ".hpp": "cpp",
            ".cs": "csharp",
            ".go": "go",
            ".rs": "rust",
            ".php": "php",
            ".rb": "ruby",
            ".swift": "swift",
            ".kt": "kotlin",
            ".scala": "scala",
            ".ipynb": "jupyter",  # Jupyter Notebook
        }
        return extension_map.get(file_extension.lower(), "unknown")

    async def exec_fallback_async(self, file_item: Dict[str, Any], exc: Exception) -> Dict[str, Any]:
        """
        è‹¥è§£æå¤±è´¥ï¼Œå°è¯•é™çº§ä¸ºä»…æå–å‡½æ•°ç­¾åæˆ–ç®€å•æ³¨é‡Š
        """
        logger.warning(f"Fallback parsing for {file_item['file_path']}: {str(exc)}")

        # ç®€å•çš„é™çº§ç­–ç•¥ï¼šæå–åŸºæœ¬ä¿¡æ¯
        return {"file_path": file_item["file_path"], "functions": [], "classes": [], "error": f"Fallback: {str(exc)}"}

    async def post_async(
        self, shared: Dict[str, Any], prep_res: List[Dict[str, Any]], exec_res: List[Dict[str, Any]]
    ) -> str:
        """
        æ•´ç†æ‰€æœ‰ç»“æœï¼Œæ›´æ–° shared.code_analysisï¼Œå®Œæˆå®æ—¶åˆ†ææŠ¥å‘Š

        Data Access:
        - Write: shared.code_analysis
        """
        # è¿‡æ»¤æ‰é”™è¯¯ç»“æœ
        valid_results = [r for r in exec_res if not r.get("error")]
        error_count = len(exec_res) - len(valid_results)

        shared["code_analysis"] = valid_results

        # ç»Ÿè®¡ä¿¡æ¯
        total_functions = sum(len(r.get("functions", [])) for r in valid_results)
        total_classes = sum(len(r.get("classes", [])) for r in valid_results)
        total_snippets = sum(len(r.get("code_snippets", [])) for r in valid_results)

        logger.info(
            f"Code parsing completed: {len(valid_results)} files, "
            f"{total_functions} functions, {total_classes} classes, "
            f"{total_snippets} code snippets, {error_count} errors"
        )

        # å®Œæˆå®æ—¶åˆ†ææŠ¥å‘Š
        await self._finalize_analysis_report(shared, valid_results, error_count)

        return "default"

    async def _finalize_analysis_report(
        self, shared: Dict[str, Any], valid_results: List[Dict[str, Any]], error_count: int
    ):
        """å®Œæˆå®æ—¶åˆ†ææŠ¥å‘Šï¼Œæ·»åŠ ç»Ÿè®¡ä¿¡æ¯å’Œå¤‡ä»½ï¼ˆmarkdown å’Œ JSONï¼‰"""
        try:
            analysis_file_path = shared.get("analysis_report_path")
            json_file_path = shared.get("analysis_json_path")

            if not analysis_file_path or not json_file_path:
                logger.warning("åˆ†ææŠ¥å‘Šæ–‡ä»¶è·¯å¾„æœªæ‰¾åˆ°ï¼Œè·³è¿‡æœ€ç»ˆåŒ–")
                return

            # ç»Ÿè®¡ä¿¡æ¯
            total_functions = sum(len(r.get("functions", [])) for r in valid_results)
            total_classes = sum(len(r.get("classes", [])) for r in valid_results)
            total_snippets = sum(len(r.get("analysis_items", [])) for r in valid_results)

            # 1. å®Œæˆ markdown æ–‡ä»¶
            with open(analysis_file_path, "a", encoding="utf-8") as f:
                f.write("\n---\n\n")
                f.write("## åˆ†æç»Ÿè®¡\n\n")
                f.write(f"- æˆåŠŸåˆ†ææ–‡ä»¶æ•°: {len(valid_results)}\n")
                f.write(f"- å¤±è´¥æ–‡ä»¶æ•°: {error_count}\n")
                f.write(f"- æ€»å‡½æ•°æ•°: {total_functions}\n")
                f.write(f"- æ€»ç±»æ•°: {total_classes}\n")
                f.write(f"- æ€»ä»£ç ç‰‡æ®µæ•°: {total_snippets}\n")
                f.write(f"- å®Œæˆæ—¶é—´: {self._get_current_time()}\n")

            # 2. å®Œæˆ JSON æ–‡ä»¶
            await self._finalize_json_report(json_file_path, valid_results, error_count)

            logger.info(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²å®Œæˆ: {analysis_file_path}")
            logger.info(f"ğŸ“„ JSON æ•°æ®æ–‡ä»¶å·²å®Œæˆ: {json_file_path}")

            # 3. åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½ç‰ˆæœ¬
            repo_info = shared.get("repo_info", {})
            repo_name = repo_info.get("name", "unknown")
            repo_results_dir = Path("./data/results") / repo_name

            timestamp = self._get_current_time().replace(":", "-").replace(" ", "_")
            backup_md_path = repo_results_dir / f"analysis_report_{timestamp}.md"
            backup_json_path = repo_results_dir / f"analysis_report_{timestamp}.json"

            # å¤åˆ¶å®Œæ•´æŠ¥å‘Šåˆ°å¤‡ä»½æ–‡ä»¶
            import shutil

            shutil.copy2(analysis_file_path, backup_md_path)
            shutil.copy2(json_file_path, backup_json_path)

            logger.info(f"ğŸ“„ å¤‡ä»½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {backup_md_path}")
            logger.info(f"ğŸ“„ å¤‡ä»½ JSON æ•°æ®å·²ä¿å­˜åˆ°: {backup_json_path}")

            # æ›´æ–°å…±äº«æ•°æ®
            shared["analysis_backup_path"] = str(backup_md_path)
            shared["analysis_json_backup_path"] = str(backup_json_path)

        except Exception as e:
            logger.error(f"âŒ å®Œæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {str(e)}")

    async def _finalize_json_report(self, json_path: str, valid_results: List[Dict[str, Any]], error_count: int):
        """å®Œæˆ JSON æŠ¥å‘Šï¼Œæ·»åŠ æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            import json

            # è¯»å–ç°æœ‰çš„ JSON æ•°æ®
            with open(json_path, "r", encoding="utf-8") as f:
                json_data = json.load(f)

            # æ›´æ–°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
            json_data["statistics"]["error_count"] = error_count
            json_data["statistics"]["completion_time"] = self._get_current_time()

            # æ·»åŠ åˆ†ææ‘˜è¦
            json_data["summary"] = {
                "total_files_processed": len(valid_results) + error_count,
                "successful_files": len(valid_results),
                "failed_files": error_count,
                "success_rate": (
                    f"{(len(valid_results) / (len(valid_results) + error_count) * 100):.1f}%"
                    if (len(valid_results) + error_count) > 0
                    else "0%"
                ),
            }

            # å†™å› JSON æ–‡ä»¶
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"âŒ å®Œæˆ JSON æŠ¥å‘Šå¤±è´¥: {str(e)}")

    async def process_files_with_llm_parallel(self, file_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ LLMParser çš„å¹¶è¡Œå¤„ç†åŠŸèƒ½æ¥åˆ†ææ–‡ä»¶
        è¿™æ˜¯ä¸€ä¸ªæ›¿ä»£ AsyncParallelBatchNode çš„é«˜æ•ˆæ–¹æ³•

        Args:
            file_items: æ–‡ä»¶é¡¹åˆ—è¡¨

        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        logger.info(f"ä½¿ç”¨ LLM å¹¶è¡Œå¤„ç† {len(file_items)} ä¸ªæ–‡ä»¶")
        logger.info(f"é…ç½®: æœ€å¤§å¹¶å‘={self.max_concurrent}, æ‰¹æ¬¡å¤§å°={self.batch_size}")

        # ä¸ºæ¯ä¸ªæ–‡ä»¶æ·»åŠ  RAG ä¸Šä¸‹æ–‡
        enhanced_file_items = []
        for file_item in file_items:
            # è·å– RAG ä¸Šä¸‹æ–‡
            context = await self._get_rag_context(file_item)

            # åˆ›å»ºå¢å¼ºçš„æ–‡ä»¶é¡¹
            enhanced_item = file_item.copy()
            enhanced_item["context"] = context
            enhanced_file_items.append(enhanced_item)

        # ä½¿ç”¨ LLMParser çš„åˆ†æ‰¹å¹¶è¡Œå¤„ç†
        results = await self.llm_parser.parse_files_in_batches(enhanced_file_items)

        return results

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime

        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def _extract_notebook_content(self, notebook_path: Path) -> str:
        """
        æå– Jupyter Notebook æ–‡ä»¶ä¸­çš„ä»£ç å†…å®¹

        Args:
            notebook_path: Notebook æ–‡ä»¶è·¯å¾„

        Returns:
            æå–çš„ä»£ç å†…å®¹å­—ç¬¦ä¸²
        """
        try:
            import json

            with open(notebook_path, "r", encoding="utf-8") as f:
                notebook = json.load(f)

            code_cells = []
            cell_index = 1

            # éå†æ‰€æœ‰å•å…ƒæ ¼
            for cell in notebook.get("cells", []):
                cell_type = cell.get("cell_type", "")

                if cell_type == "code":
                    # ä»£ç å•å…ƒæ ¼
                    source = cell.get("source", [])
                    if isinstance(source, list):
                        code_content = "".join(source)
                    else:
                        code_content = str(source)

                    # è·³è¿‡ç©ºçš„ä»£ç å•å…ƒæ ¼
                    if code_content.strip():
                        code_cells.append(f"# Cell {cell_index}\n{code_content}\n")
                        cell_index += 1

                elif cell_type == "markdown":
                    # Markdown å•å…ƒæ ¼ï¼Œä½œä¸ºæ³¨é‡ŠåŒ…å«
                    source = cell.get("source", [])
                    if isinstance(source, list):
                        markdown_content = "".join(source)
                    else:
                        markdown_content = str(source)

                    if markdown_content.strip():
                        # å°† Markdown è½¬æ¢ä¸º Python æ³¨é‡Š
                        markdown_lines = markdown_content.split("\n")
                        commented_lines = [f"# {line}" if line.strip() else "#" for line in markdown_lines]
                        code_cells.append(f"# Markdown Cell {cell_index}\n" + "\n".join(commented_lines) + "\n")
                        cell_index += 1

            # ç»„åˆæ‰€æœ‰å†…å®¹
            if code_cells:
                full_content = f"# Jupyter Notebook: {notebook_path.name}\n\n" + "\n".join(code_cells)
                return full_content
            else:
                return ""

        except Exception as e:
            logger.warning(f"Failed to extract notebook content from {notebook_path}: {str(e)}")
            return ""
