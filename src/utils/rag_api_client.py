"""
RAG API å®¢æˆ·ç«¯
ä½¿ç”¨æ‚¨æä¾›çš„ RAG API æœåŠ¡è¿›è¡Œæ–‡æ¡£å‘é‡åŒ–å’Œæ£€ç´¢
"""

import requests
import time
import asyncio
from typing import List, Dict, Any, Optional
from pathlib import Path

from .logger import logger
from .error_handler import VectorStoreError


class RAGAPIClient:
    """RAG API å®¢æˆ·ç«¯ï¼Œå‚ç…§ demo.py å®ç°"""

    def __init__(self, base_url: str):
        """
        åˆå§‹åŒ– RAG API å®¢æˆ·ç«¯

        Args:
            base_url: RAG API æœåŠ¡åœ°å€ï¼ˆå¿…éœ€å‚æ•°ï¼‰
        """
        if not base_url:
            raise ValueError("RAG API base_url is required")
        self.base_url = base_url
        self.index_name = None

    def check_health(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=10)
            if response.status_code == 200:
                logger.info("âœ… RAG API æœåŠ¡è¿è¡Œæ­£å¸¸")
                return True
            else:
                logger.error(f"âŒ RAG API æœåŠ¡å¼‚å¸¸: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"âŒ æ— æ³•è¿æ¥ RAG API æœåŠ¡: {e}")
            return False

    def create_knowledge_base(
        self, documents: List[Dict[str, Any]], vector_field: str = "content", project_name: str = None
    ) -> Optional[str]:
        """
        åˆ›å»ºçŸ¥è¯†åº“ï¼Œå‚ç…§ demo.py çš„ create_documents æ–¹æ³•

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å« titleã€contentã€category ç­‰å­—æ®µ
            vector_field: å‘é‡åŒ–å­—æ®µå
            project_name: é¡¹ç›®åç§°ï¼Œç”¨äºç´¢å¼•å‰ç¼€

        Returns:
            ç´¢å¼•åç§°ï¼Œå¤±è´¥è¿”å› None
        """
        logger.info(f"ğŸ“š æ­£åœ¨åˆ›å»ºçŸ¥è¯†åº“ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")
        if project_name:
            logger.info(f"ğŸ“‚ é¡¹ç›®åç§°: {project_name}")

        try:
            # æ„å»ºè¯·æ±‚æ•°æ®ï¼Œå‚ç…§ demo.py æ ¼å¼
            request_data = {"documents": documents, "vector_field": vector_field}

            # å¦‚æœæä¾›äº†é¡¹ç›®åç§°ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­ï¼ˆç”¨äºç”Ÿæˆå¸¦å‰ç¼€çš„ç´¢å¼•åï¼‰
            # if project_name:
            # request_data["project_name"] = project_name

            response = requests.post(
                f"{self.base_url}/documents",
                headers={"Content-Type": "application/json"},
                json=request_data,
                timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶ï¼Œå› ä¸ºå‘é‡åŒ–å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
            )

            if response.status_code == 200:
                result = response.json()
                self.index_name = result["index"]
                count = result["count"]
                logger.info(f"âœ… çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ")
                logger.info(f"ğŸ“‚ ç´¢å¼•åç§°: {self.index_name}")
                logger.info(f"ğŸ“„ æ–‡æ¡£æ•°é‡: {count}")
                return self.index_name
            else:
                error_msg = response.json() if response.content else f"HTTP {response.status_code}"
                logger.error(f"âŒ çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥: {error_msg}")
                return None

        except requests.exceptions.Timeout:
            logger.error("âŒ åˆ›å»ºçŸ¥è¯†åº“è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£æ•°é‡æˆ–ç½‘ç»œè¿æ¥")
            return None
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            return None

    def search_knowledge(
        self, query: str, index_name: str = None, vector_field: str = None, top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£ï¼Œå‚ç…§ demo.py çš„ search_documents æ–¹æ³•

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            index_name: ç´¢å¼•åç§°ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨å½“å‰ç´¢å¼•
            vector_field: å‘é‡åŒ–å­—æ®µå
            top_k: è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£æ•°é‡

        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if not index_name:
            index_name = self.index_name

        if not index_name:
            logger.error("âŒ çŸ¥è¯†åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆ›å»ºçŸ¥è¯†åº“")
            return []

        try:
            # æ„å»ºæœç´¢è¯·æ±‚ï¼Œå‚ç…§ demo.py æ ¼å¼
            vf = vector_field or getattr(self, "default_vector_field", "content")
            search_data = {"query": query, "vector_field": vf, "index": index_name, "top_k": top_k}

            response = requests.post(
                f"{self.base_url}/search", headers={"Content-Type": "application/json"}, json=search_data, timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                results = result["results"]
                total = result["total"]
                took = result["took"]

                logger.info(f"ğŸ” æœç´¢å®Œæˆ: æ‰¾åˆ° {total} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œè€—æ—¶ {took}ms")
                return results
            else:
                error_msg = response.json() if response.content else f"HTTP {response.status_code}"
                logger.error(f"âŒ æœç´¢å¤±è´¥: {error_msg}")
                return []

        except Exception as e:
            logger.error(f"âŒ æœç´¢æ—¶å‡ºé”™: {e}")
            return []

    def add_documents_to_existing_index(
        self, documents: List[Dict[str, Any]], index_name: str, vector_field: str = "content", project_name: str = None
    ) -> bool:
        """
        å‘å·²å­˜åœ¨çš„ç´¢å¼•æ·»åŠ æ–‡æ¡£

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            index_name: å·²å­˜åœ¨çš„ç´¢å¼•åç§°
            vector_field: å‘é‡åŒ–å­—æ®µå
            project_name: é¡¹ç›®åç§°ï¼ˆç”¨äºä¸€è‡´æ€§æ£€æŸ¥ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        # logger.info(f"ğŸ“ å‘ç´¢å¼• {index_name} æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£")
        # if project_name:
        #     logger.info(f"ğŸ“‚ é¡¹ç›®åç§°: {project_name}")

        try:
            request_data = {"documents": documents, "vector_field": vector_field, "index": index_name}

            # å¦‚æœæä¾›äº†é¡¹ç›®åç§°ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            # if project_name:
            # request_data["project_name"] = project_name

            response = requests.post(
                f"{self.base_url}/documents",
                headers={"Content-Type": "application/json"},
                json=request_data,
                timeout=300,
            )

            if response.status_code == 200:
                result = response.json()
                count = result["count"]
                logger.info(f"âœ… æˆåŠŸæ·»åŠ  {count} ä¸ªæ–‡æ¡£åˆ°ç´¢å¼•")
                return True
            else:
                error_msg = response.json() if response.content else f"HTTP {response.status_code}"
                logger.error(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {error_msg}")
                return False

        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return False


class RAGVectorStoreProvider:
    """
    ä½¿ç”¨ RAG API çš„å‘é‡å­˜å‚¨æä¾›è€…
    """

    def __init__(self, rag_api_url: str, vector_field: str = "content"):
        """
        åˆå§‹åŒ– RAG å‘é‡å­˜å‚¨æä¾›è€…

        Args:
            rag_api_url: RAG API æœåŠ¡åœ°å€ï¼ˆå¿…éœ€å‚æ•°ï¼‰
            vector_field: é»˜è®¤å‘é‡åŒ–å­—æ®µåï¼ˆå¯åœ¨åˆå§‹åŒ–æ—¶æŒ‡å®šï¼‰
        """
        from .vectorstore_provider import CodeSplitter
        from .config import get_config

        if not rag_api_url:
            raise ValueError("RAG API URL is required")

        self.rag_client = RAGAPIClient(rag_api_url)
        self.code_splitter = CodeSplitter()
        self.base_path = Path("./data/vectorstores")
        self.base_path.mkdir(parents=True, exist_ok=True)
        # è¯»å–æ‰¹é‡å¤§å°é…ç½®ï¼ˆ<=0 è¡¨ç¤ºä¸€æ¬¡æ€§ä¸Šä¼ ï¼‰
        self.rag_batch_size = get_config().rag_batch_size
        # é»˜è®¤å‘é‡åŒ–å­—æ®µï¼ˆå¯é€šè¿‡åˆå§‹åŒ–å‚æ•°æˆ–ç›´æ¥ä¿®æ”¹æ­¤å±æ€§æ¥æ§åˆ¶ï¼‰
        self.default_vector_field = vector_field

    def _generate_store_id(self, repo_info: Dict[str, Any]) -> str:
        """ç”Ÿæˆå‘é‡å­˜å‚¨çš„å”¯ä¸€IDï¼ˆä½¿ç”¨ä»“åº“åï¼‰"""
        full_name = repo_info.get("full_name", "unknown")
        if "/" in full_name:
            repo_name = full_name.split("/")[-1]
        else:
            repo_name = full_name
        return repo_name

    async def build_vectorstore(self, repo_path: Path, repo_info: Dict[str, Any]) -> str:
        """
        æ„å»ºå‘é‡å­˜å‚¨ï¼Œä½¿ç”¨ RAG API

        Args:
            repo_path: æœ¬åœ°ä»“åº“è·¯å¾„
            repo_info: ä»“åº“ä¿¡æ¯

        Returns:
            å‘é‡å­˜å‚¨æ ‡è¯†ï¼ˆç´¢å¼•åç§°ï¼‰
        """
        try:
            # æ£€æŸ¥ RAG API æœåŠ¡çŠ¶æ€
            if not self.rag_client.check_health():
                raise VectorStoreError("RAG API æœåŠ¡ä¸å¯ç”¨")

            store_id = self._generate_store_id(repo_info)
            logger.info(f"å¼€å§‹ä¸ºä»“åº“ {store_id} æ„å»ºå‘é‡çŸ¥è¯†åº“")

            # æ”¶é›†æ‰€æœ‰ä»£ç æ–‡ä»¶å¹¶è½¬æ¢ä¸ºæ–‡æ¡£
            documents = []
            processed_files = 0

            for file_path in self._get_code_files(repo_path):
                try:
                    elements = self.code_splitter.extract_code_elements(file_path)

                    for element in elements:
                        # æ„å»ºç¬¦åˆ RAG API è¦æ±‚çš„æ–‡æ¡£æ ¼å¼ï¼ˆæ»¡è¶³ï¼štitleã€fileã€contentã€categoryï¼‰
                        # titleï¼šä¼˜å…ˆä½¿ç”¨ç±»å/å‡½æ•°å
                        if element.get("element_type") in ("function", "class"):
                            desired_title = element.get("element_name", element.get("title", ""))
                        else:
                            desired_title = element.get("title") or element.get("element_name") or file_path.stem

                        # fileï¼šç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼ˆrepo_pathï¼‰çš„æ–‡ä»¶è·¯å¾„
                        try:
                            file_rel = str(file_path.relative_to(repo_path))
                        except Exception:
                            file_rel = element.get("file_path", str(file_path))

                        # categoryï¼šæ ¹æ®æ–‡ä»¶ç±»å‹åŒºåˆ†â€œæ–‡æ¡£/ä»£ç â€
                        doc_exts = {".md", ".mdx", ".rst", ".txt", ".adoc"}
                        desired_category = "æ–‡æ¡£" if file_path.suffix.lower() in doc_exts else "ä»£ç "

                        doc = {
                            "title": desired_title,
                            "file": file_rel,
                            "content": element["content"],
                            "category": desired_category,
                            # ä»¥ä¸‹ä¸ºå…¼å®¹/é™„åŠ å…ƒä¿¡æ¯ï¼Œä¾¿äºåç»­æ£€ç´¢ä¸è°ƒè¯•
                            "language": element.get("language"),
                            # "repo_name": repo_info.get("full_name", "unknown"),
                            "start_line": element.get("start_line", 1),
                            "end_line": element.get("end_line", 1),
                        }
                        documents.append(doc)

                    processed_files += 1
                    if processed_files % 10 == 0:
                        logger.info(f"å·²å¤„ç† {processed_files} ä¸ªæ–‡ä»¶ï¼Œæå– {len(documents)} ä¸ªä»£ç /æ–‡æ¡£ç‰‡æ®µå…ƒç´ ")

                except Exception as e:
                    logger.warning(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
                    continue

            if not documents:
                raise VectorStoreError("æœªæ‰¾åˆ°å¯å‘é‡åŒ–çš„ä»£ç æ–‡æ¡£")

            logger.info(f"æ€»å…±æå– {len(documents)} ä¸ªä»£ç /æ–‡æ¡£ç‰‡æ®µå…ƒç´ ï¼Œå¼€å§‹å‘é‡åŒ–...")

            # åˆ†æ‰¹å¤„ç†å¤§é‡æ–‡æ¡£ï¼Œé¿å…è¶…æ—¶ï¼›å¯é€šè¿‡ .env é…ç½® RAG_BATCH_SIZE
            batch_size = self.rag_batch_size
            index_name = None

            if batch_size <= 0 or batch_size >= len(documents):
                # ä¸€æ¬¡æ€§ä¸Šä¼ æ‰€æœ‰æ–‡æ¡£
                logger.info(f"ä¸€æ¬¡æ€§ä¸Šä¼ æ‰€æœ‰æ–‡æ¡£ï¼ˆå…± {len(documents)} æ¡ï¼‰")
                index_name = self.rag_client.create_knowledge_base(
                    documents=documents, vector_field="content", project_name=store_id
                )
                if not index_name:
                    raise VectorStoreError("RAG API åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥")
            else:
                for i in range(0, len(documents), batch_size):
                    batch = documents[i : i + batch_size]
                    batch_num = i // batch_size + 1
                    total_batches = (len(documents) + batch_size - 1) // batch_size

                    logger.info(f"å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹æ–‡æ¡£ ({len(batch)} ä¸ªæ–‡æ¡£)")

                    if i == 0:
                        # ç¬¬ä¸€æ‰¹ï¼šåˆ›å»ºæ–°çš„çŸ¥è¯†åº“ï¼Œä½¿ç”¨é¡¹ç›®åç§°ä½œä¸ºå‰ç¼€
                        index_name = self.rag_client.create_knowledge_base(
                            documents=batch, vector_field="content", project_name=store_id
                        )
                        if not index_name:
                            raise VectorStoreError("RAG API åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥")
                    else:
                        # åç»­æ‰¹æ¬¡ï¼šæ·»åŠ åˆ°å·²å­˜åœ¨çš„çŸ¥è¯†åº“
                        success = self.rag_client.add_documents_to_existing_index(
                            documents=batch,
                            index_name=index_name,
                            vector_field="content",
                            project_name=store_id,
                        )
                        if not success:
                            logger.warning(f"ç¬¬ {batch_num} æ‰¹æ–‡æ¡£æ·»åŠ å¤±è´¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹")

                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™æµ
                    if i + batch_size < len(documents):
                        await asyncio.sleep(1)

            if not index_name:
                raise VectorStoreError("RAG API åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥")

            # ä¿å­˜ç´¢å¼•ä¿¡æ¯åˆ°æœ¬åœ°ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
            store_path = self.base_path / store_id
            store_path.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "index_name": index_name,
                "repo_name": store_id,
                "repo_info": repo_info,
                "document_count": len(documents),
                "processed_files": processed_files,
                "rag_api_url": self.rag_client.base_url,
            }

            import json

            with open(store_path / "metadata.json", "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # ä¿å­˜ç´¢å¼•æ–‡æ¡£å†…å®¹ï¼ˆä¸æœ¬åœ°å…ƒæ•°æ®ç›¸åŒçš„æ–‡ä»¶å¤¹ä¸‹ï¼‰
            docs_path = store_path / "documents.jsonl"
            with open(docs_path, "w", encoding="utf-8") as f:
                for doc in documents:
                    f.write(json.dumps(doc, ensure_ascii=False) + "\n")

            logger.info(f"âœ… å‘é‡çŸ¥è¯†åº“æ„å»ºå®Œæˆ")
            logger.info(f"ğŸ“‚ RAG API ç´¢å¼•: {index_name}")
            logger.info(f"ğŸ“„ æ–‡æ¡£æ•°é‡: {len(documents)}")
            logger.info(f"ğŸ“ æœ¬åœ°å…ƒæ•°æ®: {store_path}")

            return index_name

        except Exception as e:
            raise VectorStoreError(f"æ„å»ºå‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")

    def _get_code_files(self, repo_path: Path) -> List[Path]:
        """è·å–æ‰€æœ‰ä»£ç æ–‡ä»¶"""
        from .file_filter import FileFilter, SUPPORTED_CODE_EXTENSIONS

        # æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶æ‰©å±•å
        doc_extensions = {".md", ".mdx", ".rst", ".txt", ".adoc"}
        allowed_extensions = SUPPORTED_CODE_EXTENSIONS | doc_extensions

        file_filter = FileFilter(repo_path)
        code_files = file_filter.scan_directory(repo_path, allowed_extensions)

        logger.info(f"æ‰¾åˆ° {len(code_files)} ä¸ªä»£ç /æ–‡æ¡£æ–‡ä»¶")
        return code_files

    async def search_similar(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        ç›¸ä¼¼æ€§æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡

        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        if not self.rag_client.index_name:
            logger.error("âŒ çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
            return []

        return self.rag_client.search_knowledge(query, top_k=k)
