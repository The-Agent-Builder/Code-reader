"""
ç»“æœå­˜å‚¨ç®¡ç†æ¨¡å—
ç®¡ç†åˆ†æç»“æœçš„ç›®å½•ç»“æ„ä¸å…ƒä¿¡æ¯è®°å½•ï¼Œæ”¯æŒJSON/MarkdownåŒæ ¼å¼å­˜å‚¨
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

from .logger import logger
from .error_handler import ResultStorageError
from .config import get_config


class ResultStorage:
    """åˆ†æç»“æœå­˜å‚¨ç®¡ç†å™¨"""

    def __init__(self, base_path: Optional[str] = None):
        if base_path:
            self.base_path = Path(base_path)
        else:
            config = get_config()
            self.base_path = config.results_path
        self.base_path.mkdir(parents=True, exist_ok=True)
        self.index_file = self.base_path / "index.json"
        self._load_index()

    def _load_index(self):
        """åŠ è½½ç´¢å¼•æ–‡ä»¶"""
        try:
            if self.index_file.exists():
                with open(self.index_file, "r", encoding="utf-8") as f:
                    self.index = json.load(f)
            else:
                self.index = {"analyses": []}

            # æ‰«ææ–‡ä»¶å¤¹å¹¶æ›´æ–°ç´¢å¼•
            self._scan_and_update_index()
        except Exception as e:
            logger.warning(f"Failed to load index file: {str(e)}")
            self.index = {"analyses": []}
            # å°è¯•é‡å»ºç´¢å¼•
            self._scan_and_update_index()

    def _save_index(self):
        """ä¿å­˜ç´¢å¼•æ–‡ä»¶"""
        try:
            with open(self.index_file, "w", encoding="utf-8") as f:
                json.dump(self.index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Failed to save index file: {str(e)}")

    def _scan_and_update_index(self):
        """æ‰«æç»“æœæ–‡ä»¶å¤¹å¹¶æ›´æ–°ç´¢å¼•"""
        try:
            logger.info("ğŸ” æ‰«æåˆ†æç»“æœæ–‡ä»¶å¤¹...")

            # è·å–ç°æœ‰ç´¢å¼•ä¸­çš„åˆ†æID
            existing_ids = {analysis.get("analysis_id") for analysis in self.index["analyses"]}

            # æ‰«ææ–‡ä»¶å¤¹
            discovered_count = 0
            for item in self.base_path.iterdir():
                if item.is_dir() and item.name != "__pycache__":
                    analysis_id = item.name

                    # å¦‚æœç´¢å¼•ä¸­å·²å­˜åœ¨ï¼Œè·³è¿‡
                    if analysis_id in existing_ids:
                        continue

                    # å°è¯•ä»æ–‡ä»¶å¤¹ä¸­åŠ è½½åˆ†æç»“æœ
                    metadata = self._load_analysis_metadata(analysis_id)
                    if metadata:
                        self.index["analyses"].append(metadata)
                        discovered_count += 1
                        logger.info(f"ğŸ“ å‘ç°åˆ†æç»“æœ: {analysis_id}")

            if discovered_count > 0:
                # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
                self.index["analyses"].sort(key=lambda x: x.get("created_at", ""), reverse=True)
                self._save_index()
                logger.info(f"âœ… å‘ç°å¹¶æ·»åŠ äº† {discovered_count} ä¸ªåˆ†æç»“æœåˆ°ç´¢å¼•")
            else:
                logger.info("ğŸ“‹ æ²¡æœ‰å‘ç°æ–°çš„åˆ†æç»“æœ")

        except Exception as e:
            logger.error(f"æ‰«æåˆ†æç»“æœæ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")

    def _load_analysis_metadata(self, analysis_id: str) -> Optional[Dict[str, Any]]:
        """ä»åˆ†ææ–‡ä»¶å¤¹ä¸­åŠ è½½å…ƒæ•°æ®"""
        try:
            analysis_dir = self.base_path / analysis_id

            # ä¼˜å…ˆå°è¯•åŠ è½½ metadata.json
            metadata_file = analysis_dir / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file, "r", encoding="utf-8") as f:
                    metadata = json.load(f)

                    # æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯æ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœä¸ºç©ºåˆ™å°è¯•é‡æ–°è®¡ç®—
                    stats = metadata.get("statistics", {})
                    total_files = stats.get("total_files", 0)

                    if total_files == 0:
                        # å°è¯•ä»analysis.jsoné‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                        enhanced_metadata = self._enhance_metadata_with_stats(metadata, analysis_dir)
                        if enhanced_metadata:
                            return enhanced_metadata

                    return metadata

            # å¦‚æœæ²¡æœ‰ metadata.jsonï¼Œå°è¯•ä»å…¶ä»–æ–‡ä»¶æ„å»ºå…ƒæ•°æ®
            return self._build_metadata_from_files(analysis_id, analysis_dir)

        except Exception as e:
            logger.warning(f"åŠ è½½åˆ†æå…ƒæ•°æ®å¤±è´¥ {analysis_id}: {str(e)}")
            return None

    def _build_metadata_from_files(self, analysis_id: str, analysis_dir: Path) -> Optional[Dict[str, Any]]:
        """ä»æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶æ„å»ºå…ƒæ•°æ®"""
        try:
            metadata = {
                "analysis_id": analysis_id,
                "repo_name": analysis_id,  # é»˜è®¤ä½¿ç”¨æ–‡ä»¶å¤¹åä½œä¸ºä»“åº“å
                "status": "completed",
                "statistics": {"total_files": 0, "total_functions": 0, "total_classes": 0, "languages": {}},
            }

            # å°è¯•ä» repo_info.json è·å–ä»“åº“ä¿¡æ¯
            repo_info_file = analysis_dir / "repo_info.json"
            if repo_info_file.exists():
                with open(repo_info_file, "r", encoding="utf-8") as f:
                    repo_info = json.load(f)
                    metadata["repo_url"] = repo_info.get("clone_url", "")
                    metadata["repo_name"] = repo_info.get("full_name", analysis_id)
                    if "languages" in repo_info:
                        metadata["statistics"]["languages"] = repo_info["languages"]

            # å°è¯•ä» analysis.json è·å–åˆ†æä¿¡æ¯
            analysis_file = analysis_dir / "analysis.json"
            if analysis_file.exists():
                with open(analysis_file, "r", encoding="utf-8") as f:
                    analysis_data = json.load(f)
                    if "analysis_time" in analysis_data:
                        metadata["analysis_time"] = analysis_data["analysis_time"]
                        metadata["created_at"] = analysis_data["analysis_time"]

                    # ç»Ÿè®¡ä»£ç åˆ†æç»“æœ
                    code_analysis = analysis_data.get("code_analysis", [])
                    if code_analysis:
                        total_functions = 0
                        total_classes = 0
                        for file_analysis in code_analysis:
                            if isinstance(file_analysis, dict):
                                total_functions += len(file_analysis.get("functions", []))
                                total_classes += len(file_analysis.get("classes", []))

                        metadata["statistics"]["total_files"] = len(code_analysis)
                        metadata["statistics"]["total_functions"] = total_functions
                        metadata["statistics"]["total_classes"] = total_classes
                    else:
                        # å¦‚æœæ²¡æœ‰è¯¦ç»†çš„ä»£ç åˆ†æç»“æœï¼Œå°è¯•ä»æœ¬åœ°ä»“åº“ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
                        local_path = analysis_data.get("local_path")
                        if local_path:
                            basic_stats = self._get_basic_file_stats(Path(local_path))
                            metadata["statistics"].update(basic_stats)

            # å¦‚æœæ²¡æœ‰æ—¶é—´ä¿¡æ¯ï¼Œä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            if "created_at" not in metadata:
                try:
                    # ä½¿ç”¨æœ€æ–°çš„åˆ†ææŠ¥å‘Šæ–‡ä»¶æ—¶é—´
                    report_files = list(analysis_dir.glob("analysis_report*.md"))
                    if report_files:
                        latest_file = max(report_files, key=lambda f: f.stat().st_mtime)
                        timestamp = datetime.fromtimestamp(latest_file.stat().st_mtime)
                        metadata["created_at"] = timestamp.isoformat()
                        metadata["analysis_time"] = timestamp.isoformat()
                    else:
                        metadata["created_at"] = datetime.now().isoformat()
                        metadata["analysis_time"] = datetime.now().isoformat()
                except:
                    metadata["created_at"] = datetime.now().isoformat()
                    metadata["analysis_time"] = datetime.now().isoformat()

            return metadata

        except Exception as e:
            logger.warning(f"æ„å»ºå…ƒæ•°æ®å¤±è´¥ {analysis_id}: {str(e)}")
            return None

    def _get_basic_file_stats(self, repo_path: Path) -> Dict[str, Any]:
        """ä»ä»“åº“è·¯å¾„è·å–åŸºæœ¬çš„æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_files": 0,
            "total_functions": 0,
            "total_classes": 0,
        }

        try:
            if not repo_path.exists():
                return stats

            # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶è¿‡æ»¤å™¨
            from .file_filter import FileFilter, SUPPORTED_CODE_EXTENSIONS

            file_filter = FileFilter(repo_path)
            code_files = file_filter.scan_directory(repo_path, SUPPORTED_CODE_EXTENSIONS)

            total_functions = 0
            total_classes = 0
            total_files = len(code_files)

            for file_path in code_files:
                # ç®€å•çš„å‡½æ•°å’Œç±»ç»Ÿè®¡ï¼ˆåŸºäºå…³é”®å­—åŒ¹é…ï¼‰
                try:
                    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                        content = f.read()

                        # æ ¹æ®æ–‡ä»¶ç±»å‹ç»Ÿè®¡å‡½æ•°å’Œç±»
                        if file_path.suffix == ".py":
                            # Python: def å’Œ class
                            import re

                            functions = re.findall(r"^\s*def\s+\w+", content, re.MULTILINE)
                            classes = re.findall(r"^\s*class\s+\w+", content, re.MULTILINE)
                            total_functions += len(functions)
                            total_classes += len(classes)
                        elif file_path.suffix in {".js", ".ts"}:
                            # JavaScript/TypeScript: function å’Œ class
                            import re

                            functions = re.findall(r"function\s+\w+|=>\s*{|\w+\s*:\s*function", content)
                            classes = re.findall(r"class\s+\w+", content)
                            total_functions += len(functions)
                            total_classes += len(classes)
                        elif file_path.suffix == ".java":
                            # Java: public/private/protected methods å’Œ class
                            import re

                            functions = re.findall(r"(public|private|protected).*?\w+\s*\([^)]*\)\s*{", content)
                            classes = re.findall(r"(public|private)?\s*class\s+\w+", content)
                            total_functions += len(functions)
                            total_classes += len(classes)
                        # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–è¯­è¨€çš„æ”¯æŒ

                except Exception as e:
                    # å¦‚æœè¯»å–æ–‡ä»¶å¤±è´¥ï¼Œè·³è¿‡ä½†ä¸å½±å“æ•´ä½“ç»Ÿè®¡
                    logger.debug(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {str(e)}")
                    continue

            stats["total_files"] = total_files
            stats["total_functions"] = total_functions
            stats["total_classes"] = total_classes

            logger.info(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡å®Œæˆ: {total_files} æ–‡ä»¶, {total_functions} å‡½æ•°, {total_classes} ç±»")

        except Exception as e:
            logger.warning(f"è·å–åŸºæœ¬æ–‡ä»¶ç»Ÿè®¡å¤±è´¥: {str(e)}")

        return stats

    def _enhance_metadata_with_stats(self, metadata: Dict[str, Any], analysis_dir: Path) -> Optional[Dict[str, Any]]:
        """å¢å¼ºå…ƒæ•°æ®ï¼Œæ·»åŠ ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # å°è¯•ä»analysis.jsonè·å–local_path
            analysis_file = analysis_dir / "analysis.json"
            if analysis_file.exists():
                with open(analysis_file, "r", encoding="utf-8") as f:
                    analysis_data = json.load(f)
                    local_path = analysis_data.get("local_path")

                    if local_path:
                        # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
                        basic_stats = self._get_basic_file_stats(Path(local_path))

                        # æ›´æ–°å…ƒæ•°æ®ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
                        if basic_stats["total_files"] > 0:
                            metadata["statistics"].update(basic_stats)
                            logger.info(f"ğŸ“Š æ›´æ–°äº† {metadata.get('analysis_id', 'Unknown')} çš„ç»Ÿè®¡ä¿¡æ¯")
                            return metadata

            return None

        except Exception as e:
            logger.warning(f"å¢å¼ºå…ƒæ•°æ®å¤±è´¥: {str(e)}")
            return None

    def _generate_analysis_id(self, repo_info: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆ†æç»“æœçš„å”¯ä¸€IDï¼ˆä½¿ç”¨ä»“åº“åï¼‰"""
        full_name = repo_info.get("full_name", "unknown")
        if "/" in full_name:
            # ä» "owner/repo" ä¸­æå– "repo"
            repo_name = full_name.split("/")[-1]
        else:
            repo_name = full_name
        return repo_name

    def _create_analysis_directory(self, analysis_id: str) -> Path:
        """åˆ›å»ºåˆ†æç»“æœç›®å½•"""
        analysis_dir = self.base_path / analysis_id
        analysis_dir.mkdir(exist_ok=True)
        return analysis_dir

    def save_analysis_result(self, shared_data: Dict[str, Any]) -> str:
        """
        ä¿å­˜åˆ†æç»“æœ

        Args:
            shared_data: åŒ…å«å®Œæ•´åˆ†æç»“æœçš„å…±äº«æ•°æ®

        Returns:
            åˆ†æç»“æœæ–‡ä»¶è·¯å¾„
        """
        try:
            # ç”Ÿæˆåˆ†æID
            analysis_id = self._generate_analysis_id(shared_data.get("repo_info", {}))
            analysis_dir = self._create_analysis_directory(analysis_id)

            # ä¿å­˜JSONæ ¼å¼ï¼ˆè¿‡æ»¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡ï¼‰
            json_path = analysis_dir / "analysis.json"
            serializable_data = self._make_serializable(shared_data)
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(serializable_data, f, ensure_ascii=False, indent=2)

            # ç”ŸæˆMarkdownæ ¼å¼
            markdown_path = analysis_dir / "analysis.md"
            markdown_content = self._generate_markdown_report(shared_data)
            with open(markdown_path, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            # ä¿å­˜å…ƒæ•°æ®
            metadata_path = analysis_dir / "metadata.json"
            metadata = self._generate_metadata(shared_data, analysis_id)
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # æ›´æ–°ç´¢å¼•
            self._update_index(metadata)

            logger.info(f"Analysis result saved to {analysis_dir}")
            return str(markdown_path)

        except Exception as e:
            import traceback

            logger.error(f"Detailed error in save_analysis_result: {traceback.format_exc()}")
            raise ResultStorageError(f"Failed to save analysis result: {str(e)}")

    def _make_serializable(self, data: Any) -> Any:
        """
        é€’å½’åœ°å°†æ•°æ®è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼
        è¿‡æ»¤æ‰å‡½æ•°ã€ç±»å®ä¾‹ç­‰ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        """
        if isinstance(data, dict):
            result = {}
            for key, value in data.items():
                # è·³è¿‡å‡½æ•°å¯¹è±¡å’Œå…¶ä»–ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
                if callable(value):
                    continue
                try:
                    result[key] = self._make_serializable(value)
                except (TypeError, ValueError):
                    # å¦‚æœæ— æ³•åºåˆ—åŒ–ï¼Œè·³è¿‡è¿™ä¸ªé”®å€¼å¯¹
                    logger.warning(f"Skipping non-serializable key: {key}")
                    continue
            return result
        elif isinstance(data, (list, tuple)):
            result = []
            for item in data:
                try:
                    result.append(self._make_serializable(item))
                except (TypeError, ValueError):
                    # è·³è¿‡ä¸å¯åºåˆ—åŒ–çš„é¡¹
                    continue
            return result
        elif isinstance(data, (str, int, float, bool, type(None))):
            return data
        else:
            # å¯¹äºå…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            try:
                # æµ‹è¯•æ˜¯å¦å¯ä»¥JSONåºåˆ—åŒ–
                json.dumps(data)
                return data
            except (TypeError, ValueError):
                # å¦‚æœä¸èƒ½åºåˆ—åŒ–ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¡¨ç¤º
                return str(data)

    def _generate_markdown_report(self, shared_data: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š"""
        repo_info = shared_data.get("repo_info", {})
        code_analysis = shared_data.get("code_analysis", [])

        # æ„å»ºMarkdownå†…å®¹
        md_lines = []

        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        repo_name = repo_info.get("full_name", "Unknown Repository")
        md_lines.append(f"# {repo_name} - ä»£ç åˆ†ææŠ¥å‘Š")
        md_lines.append("")

        # ä»“åº“ä¿¡æ¯
        md_lines.append("## ä»“åº“ä¿¡æ¯")
        md_lines.append("")
        md_lines.append(f"- **æè¿°**: {repo_info.get('description', 'N/A')}")
        md_lines.append(f"- **ä¸»è¦è¯­è¨€**: {repo_info.get('language', 'N/A')}")
        md_lines.append(f"- **åˆ›å»ºæ—¶é—´**: {repo_info.get('created_at', 'N/A')}")
        md_lines.append(f"- **æ›´æ–°æ—¶é—´**: {repo_info.get('updated_at', 'N/A')}")
        md_lines.append(f"- **Stars**: {repo_info.get('stars', 0)}")
        md_lines.append(f"- **Forks**: {repo_info.get('forks', 0)}")
        md_lines.append("")

        # è¯­è¨€ç»Ÿè®¡
        languages = repo_info.get("languages", {})
        if languages and isinstance(languages, dict):
            md_lines.append("### è¯­è¨€åˆ†å¸ƒ")
            md_lines.append("")

            # å®‰å…¨åœ°è®¡ç®—æ€»å­—èŠ‚æ•°ï¼ŒåªåŒ…å«æ•°å­—ç±»å‹çš„å€¼
            numeric_languages = {k: v for k, v in languages.items() if isinstance(v, (int, float))}
            total_bytes = sum(numeric_languages.values()) if numeric_languages else 0

            if numeric_languages:
                for lang, bytes_count in sorted(numeric_languages.items(), key=lambda x: x[1], reverse=True):
                    percentage = (bytes_count / total_bytes) * 100 if total_bytes > 0 else 0
                    md_lines.append(f"- **{lang}**: {percentage:.1f}% ({bytes_count:,} bytes)")
            else:
                # å¦‚æœæ²¡æœ‰æ•°å­—ç±»å‹çš„è¯­è¨€æ•°æ®ï¼Œæ˜¾ç¤ºåŸå§‹æ•°æ®
                for lang, value in languages.items():
                    md_lines.append(f"- **{lang}**: {value}")
            md_lines.append("")

        # åˆ†æç»Ÿè®¡
        total_functions = sum(len(file_analysis.get("functions", [])) for file_analysis in code_analysis)
        total_classes = sum(len(file_analysis.get("classes", [])) for file_analysis in code_analysis)

        md_lines.append("## åˆ†æç»Ÿè®¡")
        md_lines.append("")
        md_lines.append(f"- **åˆ†ææ–‡ä»¶æ•°**: {len(code_analysis)}")
        md_lines.append(f"- **å‡½æ•°æ€»æ•°**: {total_functions}")
        md_lines.append(f"- **ç±»æ€»æ•°**: {total_classes}")
        md_lines.append(f"- **åˆ†ææ—¶é—´**: {shared_data.get('analysis_time', 'N/A')}")
        md_lines.append("")

        # è¯¦ç»†åˆ†æç»“æœ
        md_lines.append("## è¯¦ç»†åˆ†æç»“æœ")
        md_lines.append("")

        for file_analysis in code_analysis:
            file_path = file_analysis.get("file_path", "Unknown File")
            md_lines.append(f"### {file_path}")
            md_lines.append("")

            # å‡½æ•°
            functions = file_analysis.get("functions", [])
            if functions:
                md_lines.append("#### å‡½æ•°")
                md_lines.append("")
                for func in functions:
                    md_lines.append(f"##### {func.get('title', 'Unknown Function')}")
                    md_lines.append("")
                    md_lines.append(f"**æè¿°**: {func.get('description', 'N/A')}")
                    md_lines.append("")
                    md_lines.append(f"**ä½ç½®**: {func.get('source', 'N/A')}")
                    md_lines.append("")
                    md_lines.append(f"**è¯­è¨€**: {func.get('language', 'N/A')}")
                    md_lines.append("")
                    md_lines.append("**ä»£ç **:")
                    md_lines.append("")
                    md_lines.append(f"```{func.get('language', '')}")
                    md_lines.append(func.get("code", ""))
                    md_lines.append("```")
                    md_lines.append("")

            # ç±»
            classes = file_analysis.get("classes", [])
            if classes:
                md_lines.append("#### ç±»")
                md_lines.append("")
                for cls in classes:
                    md_lines.append(f"##### {cls.get('title', 'Unknown Class')}")
                    md_lines.append("")
                    md_lines.append(f"**æè¿°**: {cls.get('description', 'N/A')}")
                    md_lines.append("")
                    md_lines.append(f"**ä½ç½®**: {cls.get('source', 'N/A')}")
                    md_lines.append("")
                    md_lines.append(f"**è¯­è¨€**: {cls.get('language', 'N/A')}")
                    md_lines.append("")
                    md_lines.append("**ä»£ç **:")
                    md_lines.append("")
                    md_lines.append(f"```{cls.get('language', '')}")
                    md_lines.append(cls.get("code", ""))
                    md_lines.append("```")
                    md_lines.append("")

        return "\n".join(md_lines)

    def _generate_metadata(self, shared_data: Dict[str, Any], analysis_id: str) -> Dict[str, Any]:
        """ç”Ÿæˆå…ƒæ•°æ®"""
        repo_info = shared_data.get("repo_info", {})
        code_analysis = shared_data.get("code_analysis", [])

        return {
            "analysis_id": analysis_id,
            "repo_url": shared_data.get("repo_url", ""),
            "repo_name": repo_info.get("full_name", "Unknown"),
            "analysis_time": shared_data.get("analysis_time", datetime.now().isoformat()),
            "status": shared_data.get("status", "completed"),
            "statistics": {
                "total_files": len(code_analysis) if isinstance(code_analysis, list) else 0,
                "total_functions": self._safe_count_items(code_analysis, "functions"),
                "total_classes": self._safe_count_items(code_analysis, "classes"),
                "languages": repo_info.get("languages", {}) if isinstance(repo_info.get("languages"), dict) else {},
                "main_language": repo_info.get("language", "") if isinstance(repo_info.get("language"), str) else "",
            },
            "created_at": datetime.now().isoformat(),
        }

    def _safe_count_items(self, code_analysis: Any, item_type: str) -> int:
        """å®‰å…¨åœ°è®¡ç®—ä»£ç åˆ†æä¸­çš„é¡¹ç›®æ•°é‡"""
        try:
            if not isinstance(code_analysis, list):
                return 0

            total = 0
            for file_analysis in code_analysis:
                if isinstance(file_analysis, dict):
                    items = file_analysis.get(item_type, [])
                    if isinstance(items, list):
                        total += len(items)
            return total
        except Exception as e:
            logger.warning(f"Error counting {item_type}: {str(e)}")
            return 0

    def _update_index(self, metadata: Dict[str, Any]):
        """æ›´æ–°ç´¢å¼•ï¼Œç¡®ä¿åŒä¸€ä¸ªä»“åº“åªä¿ç•™æœ€æ–°è®°å½•"""
        analysis_id = metadata.get("analysis_id")
        repo_name = metadata.get("repo_name")

        # ç§»é™¤åŒä¸€ä¸ªä»“åº“çš„æ—§è®°å½•
        self.index["analyses"] = [
            analysis
            for analysis in self.index["analyses"]
            if analysis.get("analysis_id") != analysis_id and analysis.get("repo_name") != repo_name
        ]

        # æ·»åŠ æ–°è®°å½•
        self.index["analyses"].append(metadata)

        # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
        self.index["analyses"].sort(key=lambda x: x.get("created_at", ""), reverse=True)

        # ä¿å­˜ç´¢å¼•
        self._save_index()

    def get_analysis_list(self, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–åˆ†æç»“æœåˆ—è¡¨"""
        # æ¸…ç†é‡å¤è®°å½•
        self._cleanup_duplicates()
        return self.index["analyses"][:limit]

    def _cleanup_duplicates(self):
        """æ¸…ç†é‡å¤çš„åˆ†æè®°å½•ï¼Œä¿ç•™æœ€æ–°çš„"""
        seen_repos = {}
        unique_analyses = []

        # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
        sorted_analyses = sorted(self.index["analyses"], key=lambda x: x.get("created_at", ""), reverse=True)

        for analysis in sorted_analyses:
            repo_name = analysis.get("repo_name", "")
            analysis_id = analysis.get("analysis_id", "")

            # æ ‡å‡†åŒ–ä»“åº“åï¼ˆè½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒï¼‰
            normalized_repo_name = repo_name.lower() if repo_name else ""

            # ä½¿ç”¨æ ‡å‡†åŒ–çš„ä»“åº“åä½œä¸ºå”¯ä¸€æ ‡è¯†
            if normalized_repo_name and normalized_repo_name not in seen_repos:
                seen_repos[normalized_repo_name] = True
                unique_analyses.append(analysis)
            elif not normalized_repo_name and analysis_id:
                # å¦‚æœæ²¡æœ‰ä»“åº“åï¼Œä½¿ç”¨analysis_idä½œä¸ºå¤‡ç”¨æ ‡è¯†
                existing_ids = [a.get("analysis_id") for a in unique_analyses]
                if analysis_id not in existing_ids:
                    unique_analyses.append(analysis)

        # å¦‚æœå‘ç°é‡å¤è®°å½•ï¼Œæ›´æ–°ç´¢å¼•
        if len(unique_analyses) != len(self.index["analyses"]):
            logger.info(f"Cleaned up duplicates: {len(self.index['analyses'])} -> {len(unique_analyses)}")
            self.index["analyses"] = unique_analyses
            self._save_index()

    def get_analysis_by_id(self, analysis_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–åˆ†æç»“æœ"""
        try:
            analysis_dir = self.base_path / analysis_id
            if not analysis_dir.exists():
                logger.warning(f"Analysis directory not found: {analysis_id}")
                return None

            # ä¼˜å…ˆä½¿ç”¨æ–°çš„æ–‡ä»¶ç»“æ„
            repo_info_path = analysis_dir / "repo_info.json"
            analysis_report_path = analysis_dir / "analysis_report.md"
            metadata_path = analysis_dir / "metadata.json"

            # å¦‚æœæ–°æ–‡ä»¶ç»“æ„å­˜åœ¨ï¼Œä½¿ç”¨æ–°ç»“æ„
            if repo_info_path.exists() and analysis_report_path.exists():
                return self._load_new_format(analysis_id, analysis_dir)

            # å¦‚æœåªæœ‰ repo_info.jsonï¼Œåˆ›å»ºåŸºæœ¬ç»“æœ
            if repo_info_path.exists():
                return self._load_minimal_format(analysis_id, analysis_dir)

            # å¦åˆ™ä½¿ç”¨æ—§çš„æ–‡ä»¶ç»“æ„
            json_path = analysis_dir / "analysis.json"
            if json_path.exists():
                return self._load_old_format(analysis_id, analysis_dir)

            logger.warning(f"No valid analysis files found for: {analysis_id}")
            return None

        except Exception as e:
            logger.error(f"Failed to load analysis {analysis_id}: {str(e)}")

        return None

    def _load_new_format(self, analysis_id: str, analysis_dir: Path) -> Dict[str, Any]:
        """åŠ è½½æ–°æ ¼å¼çš„åˆ†æç»“æœ"""
        repo_info_path = analysis_dir / "repo_info.json"
        analysis_report_path = analysis_dir / "analysis_report.md"
        metadata_path = analysis_dir / "metadata.json"

        # è¯»å–ä»“åº“ä¿¡æ¯
        with open(repo_info_path, "r", encoding="utf-8") as f:
            repo_info = json.load(f)

        # è¯»å–åˆ†ææŠ¥å‘Š
        with open(analysis_report_path, "r", encoding="utf-8") as f:
            analysis_report = f.read()

        # è§£æåˆ†ææŠ¥å‘Š
        analysis_items = self._parse_analysis_report(analysis_report)

        # è¯»å–å…ƒæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        statistics = {}
        analysis_time = None
        if metadata_path.exists():
            try:
                with open(metadata_path, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                statistics = metadata.get("statistics", {})
                analysis_time = metadata.get("created_at")

                # å¦‚æœç»Ÿè®¡ä¿¡æ¯ä¸ºç©ºï¼Œå°è¯•é‡æ–°è®¡ç®—
                if statistics.get("total_files", 0) == 0:
                    enhanced_metadata = self._enhance_metadata_with_stats(metadata, analysis_dir)
                    if enhanced_metadata:
                        statistics = enhanced_metadata.get("statistics", statistics)

            except Exception as e:
                logger.warning(f"Failed to load metadata for {analysis_id}: {str(e)}")

        # æ„å»ºè¿”å›æ•°æ®
        return {
            "analysis_id": analysis_id,
            "repo_info": repo_info,
            "code_analysis": [{"analysis_items": analysis_items}],
            "statistics": statistics,
            "analysis_time": analysis_time,
        }

    def _load_minimal_format(self, analysis_id: str, analysis_dir: Path) -> Dict[str, Any]:
        """åŠ è½½æœ€å°æ ¼å¼çš„åˆ†æç»“æœï¼ˆåªæœ‰ repo_info.jsonï¼‰"""
        repo_info_path = analysis_dir / "repo_info.json"
        metadata_path = analysis_dir / "metadata.json"

        # è¯»å–ä»“åº“ä¿¡æ¯
        with open(repo_info_path, "r", encoding="utf-8") as f:
            repo_info = json.load(f)

        # è¯»å–å…ƒæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        statistics = {}
        analysis_time = None
        if metadata_path.exists():
            try:
                with open(metadata_path, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                statistics = metadata.get("statistics", {})
                analysis_time = metadata.get("created_at")
            except Exception as e:
                logger.warning(f"Failed to load metadata for {analysis_id}: {str(e)}")

        # å¦‚æœæ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä» repo_info ä¸­æå–è¯­è¨€ä¿¡æ¯
        if not statistics and "languages" in repo_info:
            statistics = {
                "total_files": 0,
                "total_functions": 0,
                "total_classes": 0,
                "languages": repo_info["languages"],
            }

        # æ„å»ºè¿”å›æ•°æ®
        return {
            "analysis_id": analysis_id,
            "repo_info": repo_info,
            "code_analysis": [],  # ç©ºçš„ä»£ç åˆ†æç»“æœ
            "statistics": statistics,
            "analysis_time": analysis_time,
            "status": "incomplete",  # æ ‡è®°ä¸ºä¸å®Œæ•´
            "message": "åˆ†ææœªå®Œæˆæˆ–æ•°æ®ä¸å®Œæ•´",
        }

    def _parse_analysis_report(self, report_content: str) -> List[Dict[str, Any]]:
        """è§£æåˆ†ææŠ¥å‘Šï¼ŒæŒ‰ç…§ TITLEã€DESCRIPTIONã€SOURCEã€LANGUAGEã€CODE æ ¼å¼"""
        items = []
        sections = report_content.split("----------------------------------------")

        for section in sections:
            section = section.strip()
            if not section:
                continue

            item = {}
            lines = section.split("\n")
            current_field = None
            current_content = []

            for line in lines:
                # å¯¹äºä»£ç å­—æ®µï¼Œä¿æŒåŸå§‹è¡Œï¼ˆåŒ…æ‹¬ç¼©è¿›ï¼‰ï¼Œå¯¹å…¶ä»–å­—æ®µå»é™¤é¦–å°¾ç©ºç™½
                original_line = line
                stripped_line = line.strip()

                if stripped_line.startswith("TITLE:"):
                    if current_field and current_content:
                        item[current_field.lower()] = "\n".join(current_content).strip()
                    current_field = "TITLE"
                    current_content = [stripped_line[6:].strip()]
                elif stripped_line.startswith("DESCRIPTION:"):
                    if current_field and current_content:
                        item[current_field.lower()] = "\n".join(current_content).strip()
                    current_field = "DESCRIPTION"
                    current_content = [stripped_line[12:].strip()]
                elif stripped_line.startswith("SOURCE:"):
                    if current_field and current_content:
                        item[current_field.lower()] = "\n".join(current_content).strip()
                    current_field = "SOURCE"
                    current_content = [stripped_line[7:].strip()]
                elif stripped_line.startswith("LANGUAGE:"):
                    if current_field and current_content:
                        item[current_field.lower()] = "\n".join(current_content).strip()
                    current_field = "LANGUAGE"
                    current_content = [stripped_line[9:].strip()]
                elif stripped_line.startswith("CODE:"):
                    if current_field and current_content:
                        item[current_field.lower()] = "\n".join(current_content).strip()
                    current_field = "CODE"
                    current_content = []
                elif stripped_line.startswith("```") and current_field == "CODE":
                    # è·³è¿‡ä»£ç å—æ ‡è®°
                    continue
                elif current_field:
                    # å¯¹äºCODEå­—æ®µï¼Œä½¿ç”¨åŸå§‹è¡Œä¿æŒç¼©è¿›ï¼›å¯¹å…¶ä»–å­—æ®µä½¿ç”¨strippedè¡Œ
                    if current_field == "CODE":
                        current_content.append(original_line)
                    else:
                        current_content.append(stripped_line)

            # æ·»åŠ æœ€åä¸€ä¸ªå­—æ®µ
            if current_field and current_content:
                if current_field == "CODE":
                    # åªç§»é™¤ä»£ç å—æ ‡è®°ï¼Œä¿æŒåŸå§‹ç¼©è¿›å’Œç©ºè¡Œ
                    code_lines = [l for l in current_content if not l.strip().startswith("```")]
                    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºè¡Œï¼Œä½†ä¿æŒä¸­é—´çš„ç©ºè¡Œå’Œç¼©è¿›
                    while code_lines and not code_lines[0].strip():
                        code_lines.pop(0)
                    while code_lines and not code_lines[-1].strip():
                        code_lines.pop()
                    item[current_field.lower()] = "\n".join(code_lines)
                else:
                    item[current_field.lower()] = "\n".join(current_content).strip()

            if item:
                items.append(item)

        return items

    def _load_old_format(self, analysis_id: str, analysis_dir: Path) -> Dict[str, Any]:
        """åŠ è½½æ—§æ ¼å¼çš„åˆ†æç»“æœ"""
        json_path = analysis_dir / "analysis.json"
        metadata_path = analysis_dir / "metadata.json"

        # è¯»å–ä¸»è¦åˆ†ææ•°æ®
        with open(json_path, "r", encoding="utf-8") as f:
            analysis_data = json.load(f)

        # è¯»å–å…ƒæ•°æ®ï¼ˆåŒ…å«ç»Ÿè®¡ä¿¡æ¯ï¼‰
        if metadata_path.exists():
            try:
                with open(metadata_path, "r", encoding="utf-8") as f:
                    metadata = json.load(f)

                # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
                if "statistics" in metadata:
                    analysis_data["statistics"] = metadata["statistics"]

                # æ·»åŠ å…¶ä»–æœ‰ç”¨çš„å…ƒæ•°æ®
                analysis_data["analysis_time"] = metadata.get("created_at")
                analysis_data["analysis_id"] = metadata.get("analysis_id", analysis_id)

            except Exception as e:
                logger.warning(f"Failed to load metadata for {analysis_id}: {str(e)}")

        # å¤„ç†ä»£ç åˆ†ææ•°æ®ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
        if "code_analysis" in analysis_data:
            analysis_data["code_analysis"] = self._process_code_analysis(analysis_data["code_analysis"])

        # ç¡®ä¿ä»“åº“ä¿¡æ¯æ ¼å¼æ­£ç¡®
        if "repo_info" in analysis_data:
            analysis_data["repo_info"] = self._process_repo_info(analysis_data["repo_info"])

        return analysis_data

    def _process_code_analysis(self, code_analysis: Any) -> List[Dict[str, Any]]:
        """å¤„ç†ä»£ç åˆ†ææ•°æ®ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®"""
        if not isinstance(code_analysis, list):
            return []

        processed_analysis = []
        for file_analysis in code_analysis:
            if not isinstance(file_analysis, dict):
                continue

            # ç¡®ä¿æœ‰å¿…è¦çš„å­—æ®µ
            processed_file = {
                "file_path": file_analysis.get("file_path", ""),
                "functions": [],
                "classes": [],
                "analysis_items": file_analysis.get("analysis_items", []),
            }

            # å¤„ç† analysis_itemsï¼Œå°†å…¶è½¬æ¢ä¸º functions å’Œ classes
            analysis_items = file_analysis.get("analysis_items", [])
            if isinstance(analysis_items, list):
                for item in analysis_items:
                    if isinstance(item, dict):
                        # æ ¹æ®æ ‡é¢˜æˆ–æè¿°åˆ¤æ–­æ˜¯å‡½æ•°è¿˜æ˜¯ç±»
                        title = item.get("title", "")
                        if "ç±»" in title or "class" in title.lower():
                            processed_file["classes"].append(
                                {
                                    "title": item.get("title", ""),
                                    "description": item.get("description", ""),
                                    "source": item.get("source", ""),
                                    "language": item.get("language", ""),
                                    "code": item.get("code", ""),
                                }
                            )
                        else:
                            processed_file["functions"].append(
                                {
                                    "title": item.get("title", ""),
                                    "description": item.get("description", ""),
                                    "source": item.get("source", ""),
                                    "language": item.get("language", ""),
                                    "code": item.get("code", ""),
                                }
                            )

            processed_analysis.append(processed_file)

        return processed_analysis

    def _process_repo_info(self, repo_info: Any) -> Dict[str, Any]:
        """å¤„ç†ä»“åº“ä¿¡æ¯ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®"""
        if not isinstance(repo_info, dict):
            return {}

        # æ ‡å‡†åŒ–ä»“åº“ä¿¡æ¯å­—æ®µ
        processed_info = {
            "full_name": repo_info.get("full_name", repo_info.get("name", "")),
            "name": repo_info.get("name", ""),
            "description": repo_info.get("description", ""),
            "language": repo_info.get("language", repo_info.get("primary_language", "")),
            "languages": repo_info.get("languages", {}),
            "stars": repo_info.get("stars", repo_info.get("stargazers_count", 0)),
            "forks": repo_info.get("forks", repo_info.get("forks_count", 0)),
            "watchers": repo_info.get("watchers", repo_info.get("watchers_count", 0)),
            "created_at": repo_info.get("created_at", ""),
            "updated_at": repo_info.get("updated_at", ""),
            "topics": repo_info.get("topics", []),
            "license": repo_info.get("license", ""),
            "url": repo_info.get("url", ""),
            "clone_url": repo_info.get("clone_url", ""),
            "readme": repo_info.get("readme", ""),
        }

        return processed_info

    def get_analysis_result(self, analysis_id: str) -> Optional[Dict[str, Any]]:
        """è·å–åˆ†æç»“æœè¯¦æƒ…ï¼ˆåˆ«åæ–¹æ³•ï¼‰"""
        return self.get_analysis_by_id(analysis_id)

    def delete_analysis(self, analysis_id: str) -> bool:
        """åˆ é™¤åˆ†æç»“æœåŠæ‰€æœ‰ç›¸å…³æ•°æ®"""
        try:
            # è·å–åˆ†æä¿¡æ¯ä»¥ç¡®å®šä»“åº“å
            analysis_info = self.get_analysis_by_id(analysis_id)
            repo_name = None

            if analysis_info:
                repo_name = analysis_info.get("repo_name") or analysis_info.get("repo_info", {}).get("name")

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»“åº“åï¼Œå°è¯•ä»analysis_idæ¨æ–­ï¼ˆé€šå¸¸analysis_idå°±æ˜¯repo_nameï¼‰
            if not repo_name:
                repo_name = analysis_id

            deleted_items = []

            # 1. åˆ é™¤åˆ†æç»“æœç›®å½•
            analysis_dir = self.base_path / analysis_id
            if analysis_dir.exists():
                import shutil

                shutil.rmtree(analysis_dir)
                deleted_items.append(f"åˆ†æç»“æœ: {analysis_dir}")
                logger.info(f"Deleted analysis directory: {analysis_dir}")

            # 2. åˆ é™¤ä»“åº“æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if repo_name:
                from ..utils.config import get_config

                config = get_config()

                # åˆ é™¤å…‹éš†çš„ä»“åº“
                repo_path = Path(config.local_repo_path) / repo_name
                if repo_path.exists():
                    import shutil

                    shutil.rmtree(repo_path)
                    deleted_items.append(f"ä»“åº“æ–‡ä»¶: {repo_path}")
                    logger.info(f"Deleted repository: {repo_path}")

                # åˆ é™¤å‘é‡æ•°æ®åº“
                vectorstore_path = Path(config.vectorstore_path) / repo_name
                if vectorstore_path.exists():
                    import shutil

                    shutil.rmtree(vectorstore_path)
                    deleted_items.append(f"å‘é‡æ•°æ®åº“: {vectorstore_path}")
                    logger.info(f"Deleted vectorstore: {vectorstore_path}")

            # 3. ä»ç´¢å¼•ä¸­ç§»é™¤
            self.index["analyses"] = [a for a in self.index["analyses"] if a.get("analysis_id") != analysis_id]
            self._save_index()
            deleted_items.append("ç´¢å¼•è®°å½•")

            logger.info(f"Successfully deleted analysis {analysis_id}. Deleted items: {', '.join(deleted_items)}")
            return True

        except Exception as e:
            logger.error(f"Failed to delete analysis {analysis_id}: {str(e)}")
            return False
